* 
* ==> Audit <==
* |--------------|-----------------|----------|---------|---------|---------------------|---------------------|
|   Command    |      Args       | Profile  |  User   | Version |     Start Time      |      End Time       |
|--------------|-----------------|----------|---------|---------|---------------------|---------------------|
| start        | --driver=docker | minikube | vagrant | v1.32.0 | 24 Feb 24 10:08 UTC |                     |
| start        | --driver=docker | minikube | vagrant | v1.32.0 | 24 Feb 24 10:16 UTC | 24 Feb 24 10:18 UTC |
| start        |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 10:18 UTC | 24 Feb 24 10:19 UTC |
| tunnel       |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 10:19 UTC |                     |
| update-check |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 10:46 UTC | 24 Feb 24 10:46 UTC |
| stop         |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 11:58 UTC | 24 Feb 24 11:58 UTC |
| start        |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 11:59 UTC | 24 Feb 24 11:59 UTC |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 20:46 UTC |                     |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 21:05 UTC |                     |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 21:33 UTC |                     |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 21:39 UTC |                     |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 21:52 UTC |                     |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 21:56 UTC |                     |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 21:57 UTC |                     |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 22:06 UTC |                     |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 22:28 UTC |                     |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 22:32 UTC |                     |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 22:40 UTC |                     |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 22:43 UTC |                     |
| tunnel       |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 22:44 UTC |                     |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 22:46 UTC |                     |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 22:51 UTC |                     |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 22:51 UTC |                     |
| ip           |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 22:57 UTC | 24 Feb 24 22:57 UTC |
| stop         |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 23:02 UTC | 24 Feb 24 23:02 UTC |
| start        |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 23:03 UTC | 24 Feb 24 23:03 UTC |
| tunnel       |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 23:04 UTC |                     |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 23:05 UTC |                     |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 23:18 UTC |                     |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 23:36 UTC | 24 Feb 24 23:47 UTC |
| ssh          |                 | minikube | vagrant | v1.32.0 | 24 Feb 24 23:48 UTC | 24 Feb 24 23:53 UTC |
| ssh          |                 | minikube | vagrant | v1.32.0 | 25 Feb 24 08:17 UTC |                     |
| tunnel       |                 | minikube | vagrant | v1.32.0 | 25 Feb 24 08:18 UTC |                     |
| addons       | enable ingress  | minikube | vagrant | v1.32.0 | 25 Feb 24 08:31 UTC | 25 Feb 24 08:32 UTC |
| tunnel       |                 | minikube | vagrant | v1.32.0 | 25 Feb 24 08:33 UTC |                     |
| tunnel       | stop            | minikube | vagrant | v1.32.0 | 25 Feb 24 08:46 UTC |                     |
| ssh          |                 | minikube | vagrant | v1.32.0 | 25 Feb 24 08:49 UTC | 25 Feb 24 08:50 UTC |
| ssh          |                 | minikube | vagrant | v1.32.0 | 25 Feb 24 09:10 UTC | 25 Feb 24 09:15 UTC |
| ssh          |                 | minikube | vagrant | v1.32.0 | 25 Feb 24 09:20 UTC | 25 Feb 24 09:21 UTC |
| tunnel       | --cleanup       | minikube | vagrant | v1.32.0 | 25 Feb 24 09:22 UTC |                     |
| stop         |                 | minikube | vagrant | v1.32.0 | 25 Feb 24 09:22 UTC | 25 Feb 24 09:22 UTC |
| start        |                 | minikube | vagrant | v1.32.0 | 25 Feb 24 09:22 UTC | 25 Feb 24 09:22 UTC |
| ip           |                 | minikube | vagrant | v1.32.0 | 25 Feb 24 09:23 UTC | 25 Feb 24 09:23 UTC |
| addons       | list            | minikube | vagrant | v1.32.0 | 25 Feb 24 09:27 UTC | 25 Feb 24 09:27 UTC |
| addons       | enable kube-dns | minikube | vagrant | v1.32.0 | 25 Feb 24 09:30 UTC |                     |
| stop         |                 | minikube | vagrant | v1.32.0 | 25 Feb 24 09:31 UTC | 25 Feb 24 09:31 UTC |
| start        | --driver=docker | minikube | vagrant | v1.32.0 | 25 Feb 24 09:31 UTC | 25 Feb 24 09:32 UTC |
| addons       | enable kube-dns | minikube | vagrant | v1.32.0 | 25 Feb 24 09:32 UTC |                     |
|--------------|-----------------|----------|---------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/02/25 09:31:35
Running on machine: minikube
Binary: Built with gc go1.21.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0225 09:31:35.431465  745092 out.go:296] Setting OutFile to fd 1 ...
I0225 09:31:35.431573  745092 out.go:348] isatty.IsTerminal(1) = true
I0225 09:31:35.431575  745092 out.go:309] Setting ErrFile to fd 2...
I0225 09:31:35.431602  745092 out.go:348] isatty.IsTerminal(2) = true
I0225 09:31:35.431712  745092 root.go:338] Updating PATH: /home/vagrant/.minikube/bin
W0225 09:31:35.431776  745092 root.go:314] Error reading config file at /home/vagrant/.minikube/config/config.json: open /home/vagrant/.minikube/config/config.json: no such file or directory
I0225 09:31:35.431908  745092 out.go:303] Setting JSON to false
I0225 09:31:35.438636  745092 start.go:128] hostinfo: {"hostname":"minikube","uptime":37701,"bootTime":1708815795,"procs":94,"os":"linux","platform":"debian","platformFamily":"debian","platformVersion":"12.2","kernelVersion":"6.1.0-13-amd64","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"8efa94ff-517a-4b4d-ab3d-964b33611d9b"}
I0225 09:31:35.438670  745092 start.go:138] virtualization: kvm host
I0225 09:31:35.439733  745092 out.go:177] 😄  minikube v1.32.0 on Debian 12.2
I0225 09:31:35.441035  745092 notify.go:220] Checking for updates...
I0225 09:31:35.441414  745092 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0225 09:31:35.441754  745092 driver.go:378] Setting default libvirt URI to qemu:///system
I0225 09:31:35.456091  745092 docker.go:122] docker version: linux-25.0.3:Docker Engine - Community
I0225 09:31:35.456145  745092 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0225 09:31:35.515233  745092 info.go:266] docker info: {ID:338978a1-bd95-4cec-bfd0-f2257a37fed8 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:28 OomKillDisable:false NGoroutines:45 SystemTime:2024-02-25 09:31:35.508350846 +0000 UTC LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.1.0-13-amd64 OperatingSystem:Debian GNU/Linux 12 (bookworm) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:6064132096 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:minikube Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.5]] Warnings:<nil>}}
I0225 09:31:35.515285  745092 docker.go:295] overlay module found
I0225 09:31:35.516323  745092 out.go:177] ✨  Using the docker driver based on existing profile
I0225 09:31:35.517226  745092 start.go:298] selected driver: docker
I0225 09:31:35.517230  745092 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/vagrant:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0225 09:31:35.517269  745092 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0225 09:31:35.517317  745092 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0225 09:31:35.557523  745092 info.go:266] docker info: {ID:338978a1-bd95-4cec-bfd0-f2257a37fed8 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:28 OomKillDisable:false NGoroutines:45 SystemTime:2024-02-25 09:31:35.55151793 +0000 UTC LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.1.0-13-amd64 OperatingSystem:Debian GNU/Linux 12 (bookworm) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:6064132096 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:minikube Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.5]] Warnings:<nil>}}
I0225 09:31:35.557740  745092 cni.go:84] Creating CNI manager for ""
I0225 09:31:35.557754  745092 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0225 09:31:35.557761  745092 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/vagrant:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0225 09:31:35.558883  745092 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I0225 09:31:35.559763  745092 cache.go:121] Beginning downloading kic base image for docker with docker
I0225 09:31:35.560529  745092 out.go:177] 🚜  Pulling base image ...
I0225 09:31:35.561484  745092 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0225 09:31:35.561503  745092 preload.go:148] Found local preload: /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0225 09:31:35.561507  745092 cache.go:56] Caching tarball of preloaded images
I0225 09:31:35.561549  745092 preload.go:174] Found /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0225 09:31:35.561554  745092 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0225 09:31:35.561618  745092 profile.go:148] Saving config to /home/vagrant/.minikube/profiles/minikube/config.json ...
I0225 09:31:35.561821  745092 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0225 09:31:35.572291  745092 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0225 09:31:35.572301  745092 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0225 09:31:35.572312  745092 cache.go:194] Successfully downloaded all kic artifacts
I0225 09:31:35.572333  745092 start.go:365] acquiring machines lock for minikube: {Name:mk50794f3b668552bcb175548a808224fc99ceb9 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0225 09:31:35.572388  745092 start.go:369] acquired machines lock for "minikube" in 28.349µs
I0225 09:31:35.572398  745092 start.go:96] Skipping create...Using existing machine configuration
I0225 09:31:35.572400  745092 fix.go:54] fixHost starting: 
I0225 09:31:35.572535  745092 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0225 09:31:35.583003  745092 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0225 09:31:35.583022  745092 fix.go:128] unexpected machine state, will restart: <nil>
I0225 09:31:35.584229  745092 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0225 09:31:35.585156  745092 cli_runner.go:164] Run: docker start minikube
I0225 09:31:35.870514  745092 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0225 09:31:35.894784  745092 kic.go:430] container "minikube" state is running.
I0225 09:31:35.895206  745092 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0225 09:31:35.909134  745092 profile.go:148] Saving config to /home/vagrant/.minikube/profiles/minikube/config.json ...
I0225 09:31:35.909633  745092 machine.go:88] provisioning docker machine ...
I0225 09:31:35.909644  745092 ubuntu.go:169] provisioning hostname "minikube"
I0225 09:31:35.909668  745092 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:31:35.923642  745092 main.go:141] libmachine: Using SSH client type: native
I0225 09:31:35.923919  745092 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32782 <nil> <nil>}
I0225 09:31:35.923925  745092 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0225 09:31:35.924359  745092 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:50936->127.0.0.1:32782: read: connection reset by peer
I0225 09:31:39.097156  745092 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0225 09:31:39.097269  745092 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:31:39.120009  745092 main.go:141] libmachine: Using SSH client type: native
I0225 09:31:39.120355  745092 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32782 <nil> <nil>}
I0225 09:31:39.120368  745092 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0225 09:31:39.249565  745092 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0225 09:31:39.249598  745092 ubuntu.go:175] set auth options {CertDir:/home/vagrant/.minikube CaCertPath:/home/vagrant/.minikube/certs/ca.pem CaPrivateKeyPath:/home/vagrant/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/vagrant/.minikube/machines/server.pem ServerKeyPath:/home/vagrant/.minikube/machines/server-key.pem ClientKeyPath:/home/vagrant/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/vagrant/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/vagrant/.minikube}
I0225 09:31:39.249632  745092 ubuntu.go:177] setting up certificates
I0225 09:31:39.249650  745092 provision.go:83] configureAuth start
I0225 09:31:39.249755  745092 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0225 09:31:39.289301  745092 provision.go:138] copyHostCerts
I0225 09:31:39.289350  745092 exec_runner.go:144] found /home/vagrant/.minikube/cert.pem, removing ...
I0225 09:31:39.289357  745092 exec_runner.go:203] rm: /home/vagrant/.minikube/cert.pem
I0225 09:31:39.289436  745092 exec_runner.go:151] cp: /home/vagrant/.minikube/certs/cert.pem --> /home/vagrant/.minikube/cert.pem (1123 bytes)
I0225 09:31:39.289558  745092 exec_runner.go:144] found /home/vagrant/.minikube/key.pem, removing ...
I0225 09:31:39.289562  745092 exec_runner.go:203] rm: /home/vagrant/.minikube/key.pem
I0225 09:31:39.289595  745092 exec_runner.go:151] cp: /home/vagrant/.minikube/certs/key.pem --> /home/vagrant/.minikube/key.pem (1675 bytes)
I0225 09:31:39.289662  745092 exec_runner.go:144] found /home/vagrant/.minikube/ca.pem, removing ...
I0225 09:31:39.289666  745092 exec_runner.go:203] rm: /home/vagrant/.minikube/ca.pem
I0225 09:31:39.289695  745092 exec_runner.go:151] cp: /home/vagrant/.minikube/certs/ca.pem --> /home/vagrant/.minikube/ca.pem (1078 bytes)
I0225 09:31:39.289770  745092 provision.go:112] generating server cert: /home/vagrant/.minikube/machines/server.pem ca-key=/home/vagrant/.minikube/certs/ca.pem private-key=/home/vagrant/.minikube/certs/ca-key.pem org=vagrant.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0225 09:31:39.438034  745092 provision.go:172] copyRemoteCerts
I0225 09:31:39.438064  745092 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0225 09:31:39.438085  745092 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:31:39.448119  745092 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32782 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0225 09:31:39.546399  745092 ssh_runner.go:362] scp /home/vagrant/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0225 09:31:39.566195  745092 ssh_runner.go:362] scp /home/vagrant/.minikube/machines/server.pem --> /etc/docker/server.pem (1204 bytes)
I0225 09:31:39.585645  745092 ssh_runner.go:362] scp /home/vagrant/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0225 09:31:39.605018  745092 provision.go:86] duration metric: configureAuth took 355.358718ms
I0225 09:31:39.605054  745092 ubuntu.go:193] setting minikube options for container-runtime
I0225 09:31:39.605155  745092 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0225 09:31:39.605179  745092 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:31:39.615306  745092 main.go:141] libmachine: Using SSH client type: native
I0225 09:31:39.615532  745092 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32782 <nil> <nil>}
I0225 09:31:39.615538  745092 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0225 09:31:39.733347  745092 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0225 09:31:39.733354  745092 ubuntu.go:71] root file system type: overlay
I0225 09:31:39.733429  745092 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0225 09:31:39.733461  745092 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:31:39.748122  745092 main.go:141] libmachine: Using SSH client type: native
I0225 09:31:39.748358  745092 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32782 <nil> <nil>}
I0225 09:31:39.748400  745092 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0225 09:31:39.909226  745092 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0225 09:31:39.909273  745092 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:31:39.919532  745092 main.go:141] libmachine: Using SSH client type: native
I0225 09:31:39.919771  745092 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32782 <nil> <nil>}
I0225 09:31:39.919780  745092 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0225 09:31:40.065081  745092 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0225 09:31:40.065108  745092 machine.go:91] provisioned docker machine in 4.155464538s
I0225 09:31:40.065121  745092 start.go:300] post-start starting for "minikube" (driver="docker")
I0225 09:31:40.065138  745092 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0225 09:31:40.065232  745092 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0225 09:31:40.065288  745092 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:31:40.087630  745092 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32782 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0225 09:31:40.179127  745092 ssh_runner.go:195] Run: cat /etc/os-release
I0225 09:31:40.181900  745092 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0225 09:31:40.181919  745092 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0225 09:31:40.181924  745092 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0225 09:31:40.181928  745092 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0225 09:31:40.181933  745092 filesync.go:126] Scanning /home/vagrant/.minikube/addons for local assets ...
I0225 09:31:40.181962  745092 filesync.go:126] Scanning /home/vagrant/.minikube/files for local assets ...
I0225 09:31:40.181975  745092 start.go:303] post-start completed in 116.848103ms
I0225 09:31:40.181995  745092 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0225 09:31:40.182011  745092 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:31:40.193887  745092 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32782 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0225 09:31:40.291435  745092 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0225 09:31:40.294875  745092 fix.go:56] fixHost completed within 4.722472159s
I0225 09:31:40.294881  745092 start.go:83] releasing machines lock for "minikube", held for 4.722488667s
I0225 09:31:40.294911  745092 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0225 09:31:40.305473  745092 ssh_runner.go:195] Run: cat /version.json
I0225 09:31:40.305496  745092 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:31:40.305612  745092 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0225 09:31:40.305628  745092 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:31:40.317114  745092 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32782 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0225 09:31:40.318830  745092 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32782 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0225 09:31:40.552451  745092 ssh_runner.go:195] Run: systemctl --version
I0225 09:31:40.566184  745092 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0225 09:31:40.569974  745092 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0225 09:31:40.585789  745092 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0225 09:31:40.585823  745092 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0225 09:31:40.593378  745092 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0225 09:31:40.593386  745092 start.go:472] detecting cgroup driver to use...
I0225 09:31:40.593402  745092 detect.go:199] detected "systemd" cgroup driver on host os
I0225 09:31:40.593459  745092 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0225 09:31:40.607298  745092 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0225 09:31:40.615428  745092 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0225 09:31:40.623405  745092 containerd.go:145] configuring containerd to use "systemd" as cgroup driver...
I0225 09:31:40.623431  745092 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0225 09:31:40.631364  745092 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0225 09:31:40.639133  745092 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0225 09:31:40.647287  745092 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0225 09:31:40.655158  745092 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0225 09:31:40.662481  745092 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0225 09:31:40.670227  745092 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0225 09:31:40.677865  745092 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0225 09:31:40.686327  745092 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0225 09:31:40.757373  745092 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0225 09:31:40.841530  745092 start.go:472] detecting cgroup driver to use...
I0225 09:31:40.841551  745092 detect.go:199] detected "systemd" cgroup driver on host os
I0225 09:31:40.841574  745092 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0225 09:31:40.863054  745092 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0225 09:31:40.863110  745092 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0225 09:31:40.881819  745092 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0225 09:31:40.896554  745092 ssh_runner.go:195] Run: which cri-dockerd
I0225 09:31:40.899910  745092 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0225 09:31:40.909564  745092 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0225 09:31:40.925916  745092 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0225 09:31:41.049421  745092 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0225 09:31:41.126092  745092 docker.go:560] configuring docker to use "systemd" as cgroup driver...
I0225 09:31:41.126154  745092 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0225 09:31:41.140625  745092 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0225 09:31:41.220007  745092 ssh_runner.go:195] Run: sudo systemctl restart docker
I0225 09:31:42.218588  745092 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0225 09:31:42.289246  745092 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0225 09:31:42.354308  745092 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0225 09:31:42.419438  745092 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0225 09:31:42.488701  745092 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0225 09:31:42.514051  745092 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0225 09:31:42.585242  745092 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0225 09:31:42.665079  745092 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0225 09:31:42.665108  745092 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0225 09:31:42.668130  745092 start.go:540] Will wait 60s for crictl version
I0225 09:31:42.668155  745092 ssh_runner.go:195] Run: which crictl
I0225 09:31:42.670977  745092 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0225 09:31:42.712144  745092 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0225 09:31:42.712175  745092 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0225 09:31:42.728799  745092 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0225 09:31:42.745733  745092 out.go:204] 🐳  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0225 09:31:42.745792  745092 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0225 09:31:42.755920  745092 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0225 09:31:42.759034  745092 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0225 09:31:42.767957  745092 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0225 09:31:42.767981  745092 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0225 09:31:42.783996  745092 docker.go:671] Got preloaded images: -- stdout --
curlimages/curl:latest
tulamelki/neweng:latest
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0225 09:31:42.784005  745092 docker.go:601] Images already preloaded, skipping extraction
I0225 09:31:42.784034  745092 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0225 09:31:42.797182  745092 docker.go:671] Got preloaded images: -- stdout --
curlimages/curl:latest
tulamelki/neweng:latest
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0225 09:31:42.797190  745092 cache_images.go:84] Images are preloaded, skipping loading
I0225 09:31:42.797218  745092 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0225 09:31:42.844037  745092 cni.go:84] Creating CNI manager for ""
I0225 09:31:42.844046  745092 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0225 09:31:42.844057  745092 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0225 09:31:42.844068  745092 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0225 09:31:42.844137  745092 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0225 09:31:42.844174  745092 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0225 09:31:42.844201  745092 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0225 09:31:42.851764  745092 binaries.go:44] Found k8s binaries, skipping transfer
I0225 09:31:42.851800  745092 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0225 09:31:42.858875  745092 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0225 09:31:42.873650  745092 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0225 09:31:42.889297  745092 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2090 bytes)
I0225 09:31:42.904265  745092 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0225 09:31:42.907108  745092 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0225 09:31:42.916327  745092 certs.go:56] Setting up /home/vagrant/.minikube/profiles/minikube for IP: 192.168.49.2
I0225 09:31:42.916338  745092 certs.go:190] acquiring lock for shared ca certs: {Name:mk99734a69f246b009342ee30e5dd25cb3da1093 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0225 09:31:42.916421  745092 certs.go:199] skipping minikubeCA CA generation: /home/vagrant/.minikube/ca.key
I0225 09:31:42.916447  745092 certs.go:199] skipping proxyClientCA CA generation: /home/vagrant/.minikube/proxy-client-ca.key
I0225 09:31:42.916491  745092 certs.go:315] skipping minikube-user signed cert generation: /home/vagrant/.minikube/profiles/minikube/client.key
I0225 09:31:42.916522  745092 certs.go:315] skipping minikube signed cert generation: /home/vagrant/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0225 09:31:42.916547  745092 certs.go:315] skipping aggregator signed cert generation: /home/vagrant/.minikube/profiles/minikube/proxy-client.key
I0225 09:31:42.916621  745092 certs.go:437] found cert: /home/vagrant/.minikube/certs/home/vagrant/.minikube/certs/ca-key.pem (1679 bytes)
I0225 09:31:42.916638  745092 certs.go:437] found cert: /home/vagrant/.minikube/certs/home/vagrant/.minikube/certs/ca.pem (1078 bytes)
I0225 09:31:42.916654  745092 certs.go:437] found cert: /home/vagrant/.minikube/certs/home/vagrant/.minikube/certs/cert.pem (1123 bytes)
I0225 09:31:42.916669  745092 certs.go:437] found cert: /home/vagrant/.minikube/certs/home/vagrant/.minikube/certs/key.pem (1675 bytes)
I0225 09:31:42.916996  745092 ssh_runner.go:362] scp /home/vagrant/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0225 09:31:42.938337  745092 ssh_runner.go:362] scp /home/vagrant/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0225 09:31:42.961816  745092 ssh_runner.go:362] scp /home/vagrant/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0225 09:31:42.988819  745092 ssh_runner.go:362] scp /home/vagrant/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0225 09:31:43.013358  745092 ssh_runner.go:362] scp /home/vagrant/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0225 09:31:43.035296  745092 ssh_runner.go:362] scp /home/vagrant/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0225 09:31:43.056770  745092 ssh_runner.go:362] scp /home/vagrant/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0225 09:31:43.077914  745092 ssh_runner.go:362] scp /home/vagrant/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0225 09:31:43.098979  745092 ssh_runner.go:362] scp /home/vagrant/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0225 09:31:43.119035  745092 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0225 09:31:43.133827  745092 ssh_runner.go:195] Run: openssl version
I0225 09:31:43.138037  745092 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0225 09:31:43.145848  745092 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0225 09:31:43.149192  745092 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Feb 24 10:18 /usr/share/ca-certificates/minikubeCA.pem
I0225 09:31:43.149297  745092 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0225 09:31:43.154748  745092 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0225 09:31:43.162483  745092 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0225 09:31:43.165507  745092 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0225 09:31:43.170514  745092 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0225 09:31:43.175618  745092 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0225 09:31:43.181247  745092 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0225 09:31:43.186711  745092 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0225 09:31:43.191893  745092 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0225 09:31:43.196786  745092 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/vagrant:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0225 09:31:43.196842  745092 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0225 09:31:43.209754  745092 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0225 09:31:43.217244  745092 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0225 09:31:43.217249  745092 kubeadm.go:636] restartCluster start
I0225 09:31:43.217272  745092 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0225 09:31:43.224174  745092 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0225 09:31:43.224422  745092 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in /home/vagrant/.kube/config
I0225 09:31:43.224475  745092 kubeconfig.go:146] "minikube" context is missing from /home/vagrant/.kube/config - will repair!
I0225 09:31:43.224621  745092 lock.go:35] WriteFile acquiring /home/vagrant/.kube/config: {Name:mk584b224ce915a9a9ad34e6e788268489afc021 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0225 09:31:43.225489  745092 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0225 09:31:43.232442  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:43.232463  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:43.240487  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:43.240492  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:43.240512  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:43.248594  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:43.749739  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:43.749871  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:43.789859  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:44.249270  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:44.249394  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:44.295467  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:44.748973  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:44.749047  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:44.786711  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:45.249399  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:45.249523  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:45.296401  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:45.749387  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:45.749484  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:45.767916  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:46.248816  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:46.248947  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:46.288353  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:46.749215  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:46.749504  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:46.787191  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:47.249756  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:47.250018  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:47.292429  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:47.749672  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:47.749997  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:47.790277  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:48.248931  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:48.249192  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:48.290727  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:48.748681  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:48.749037  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:48.790661  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:49.249364  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:49.249534  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:49.262771  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:49.748962  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:49.749083  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:49.795284  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:50.248822  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:50.248940  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:50.293459  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:50.749386  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:50.749458  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:50.772409  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:51.249528  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:51.249677  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:51.293626  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:51.749410  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:51.749536  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:51.791707  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:52.250906  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:52.251304  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:52.296181  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:52.749193  745092 api_server.go:166] Checking apiserver status ...
I0225 09:31:52.749498  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:31:52.787787  745092 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:31:53.233164  745092 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0225 09:31:53.233252  745092 kubeadm.go:1128] stopping kube-system containers ...
I0225 09:31:53.233571  745092 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0225 09:31:53.271109  745092 docker.go:469] Stopping containers: [a61056ae3650 0baf065f27c5 81a58d82a0fc 5e706d51b2c2 4d8ee3b7c477 45b7a2eacd32 8987011dea02 67700306ce16 4a616b73cec7 372b1bb82a5a fbebf49eaa92 4b81ae6c3369 81dac51431a7 36257e0c48a1 44ae95d353f2 f517c4d428d0 b2e4ce09d5cb 6c884caf7982 87afe8fa23d4 7628c6d91293 252d9584b660 4aa71685d627 2c59d6a2b6dd 6d417a192731 a4b95480b009 7205e6dab762 c2f8f484c8d0]
I0225 09:31:53.271160  745092 ssh_runner.go:195] Run: docker stop a61056ae3650 0baf065f27c5 81a58d82a0fc 5e706d51b2c2 4d8ee3b7c477 45b7a2eacd32 8987011dea02 67700306ce16 4a616b73cec7 372b1bb82a5a fbebf49eaa92 4b81ae6c3369 81dac51431a7 36257e0c48a1 44ae95d353f2 f517c4d428d0 b2e4ce09d5cb 6c884caf7982 87afe8fa23d4 7628c6d91293 252d9584b660 4aa71685d627 2c59d6a2b6dd 6d417a192731 a4b95480b009 7205e6dab762 c2f8f484c8d0
I0225 09:31:53.288852  745092 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0225 09:31:53.298990  745092 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0225 09:31:53.306584  745092 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Feb 24 10:18 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Feb 25 09:22 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Feb 24 10:18 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Feb 25 09:22 /etc/kubernetes/scheduler.conf

I0225 09:31:53.306639  745092 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0225 09:31:53.313928  745092 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0225 09:31:53.321132  745092 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0225 09:31:53.327945  745092 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0225 09:31:53.327966  745092 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0225 09:31:53.334562  745092 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0225 09:31:53.341408  745092 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0225 09:31:53.341428  745092 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0225 09:31:53.348184  745092 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0225 09:31:53.355007  745092 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0225 09:31:53.355021  745092 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0225 09:31:53.407608  745092 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0225 09:31:54.310261  745092 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0225 09:31:54.444036  745092 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0225 09:31:54.492307  745092 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0225 09:31:54.567237  745092 api_server.go:52] waiting for apiserver process to appear ...
I0225 09:31:54.567268  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:31:54.580469  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:31:55.093287  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:31:55.600550  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:31:55.635858  745092 api_server.go:72] duration metric: took 1.068621972s to wait for apiserver process to appear ...
I0225 09:31:55.635867  745092 api_server.go:88] waiting for apiserver healthz status ...
I0225 09:31:55.635875  745092 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0225 09:31:57.949486  745092 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0225 09:31:57.949495  745092 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0225 09:31:57.949502  745092 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0225 09:31:57.973053  745092 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0225 09:31:57.973064  745092 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0225 09:31:58.475927  745092 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0225 09:31:58.484186  745092 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0225 09:31:58.484211  745092 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0225 09:31:58.974146  745092 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0225 09:31:58.978894  745092 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0225 09:31:58.978905  745092 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0225 09:31:59.473144  745092 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0225 09:31:59.476121  745092 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0225 09:31:59.483218  745092 api_server.go:141] control plane version: v1.28.3
I0225 09:31:59.483229  745092 api_server.go:131] duration metric: took 3.84735949s to wait for apiserver health ...
I0225 09:31:59.483234  745092 cni.go:84] Creating CNI manager for ""
I0225 09:31:59.483240  745092 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0225 09:31:59.484388  745092 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0225 09:31:59.485209  745092 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0225 09:31:59.497824  745092 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0225 09:31:59.520451  745092 system_pods.go:43] waiting for kube-system pods to appear ...
I0225 09:31:59.535775  745092 system_pods.go:59] 7 kube-system pods found
I0225 09:31:59.535786  745092 system_pods.go:61] "coredns-5dd5756b68-hdxcg" [3476c5b5-72c2-4358-aa08-c9d6f7c7ba94] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0225 09:31:59.535790  745092 system_pods.go:61] "etcd-minikube" [92e58708-2606-4d06-bb7e-d6122eca186e] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0225 09:31:59.535793  745092 system_pods.go:61] "kube-apiserver-minikube" [e20a0755-f590-40a9-b9b9-3bfb18d67518] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0225 09:31:59.535796  745092 system_pods.go:61] "kube-controller-manager-minikube" [87617930-059e-4547-a83a-8e6e26322c25] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0225 09:31:59.535799  745092 system_pods.go:61] "kube-proxy-cfj6z" [5d8bb303-bddf-441c-91b5-796fff7af79d] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0225 09:31:59.535802  745092 system_pods.go:61] "kube-scheduler-minikube" [5bfbbdb4-39bc-4799-8549-354a523f358b] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0225 09:31:59.535805  745092 system_pods.go:61] "storage-provisioner" [4f4796b2-7736-451c-8980-4101c03710de] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0225 09:31:59.535809  745092 system_pods.go:74] duration metric: took 15.349744ms to wait for pod list to return data ...
I0225 09:31:59.535814  745092 node_conditions.go:102] verifying NodePressure condition ...
I0225 09:31:59.539061  745092 node_conditions.go:122] node storage ephemeral capacity is 39019864Ki
I0225 09:31:59.539073  745092 node_conditions.go:123] node cpu capacity is 2
I0225 09:31:59.539089  745092 node_conditions.go:105] duration metric: took 3.262082ms to run NodePressure ...
I0225 09:31:59.539100  745092 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0225 09:31:59.867419  745092 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0225 09:31:59.879369  745092 ops.go:34] apiserver oom_adj: -16
I0225 09:31:59.879377  745092 kubeadm.go:640] restartCluster took 16.662124385s
I0225 09:31:59.879385  745092 kubeadm.go:406] StartCluster complete in 16.68259949s
I0225 09:31:59.879395  745092 settings.go:142] acquiring lock: {Name:mkb2c3059065ecb0ccdda4c7ff3af85f8f0082c2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0225 09:31:59.879474  745092 settings.go:150] Updating kubeconfig:  /home/vagrant/.kube/config
I0225 09:31:59.879830  745092 lock.go:35] WriteFile acquiring /home/vagrant/.kube/config: {Name:mk584b224ce915a9a9ad34e6e788268489afc021 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0225 09:31:59.880058  745092 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0225 09:31:59.880199  745092 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0225 09:31:59.880231  745092 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:true ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0225 09:31:59.880264  745092 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0225 09:31:59.880268  745092 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0225 09:31:59.880271  745092 addons.go:240] addon storage-provisioner should already be in state true
I0225 09:31:59.880293  745092 host.go:66] Checking if "minikube" exists ...
I0225 09:31:59.880501  745092 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0225 09:31:59.880522  745092 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0225 09:31:59.880535  745092 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0225 09:31:59.880662  745092 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0225 09:31:59.887292  745092 addons.go:69] Setting ingress=true in profile "minikube"
I0225 09:31:59.887315  745092 addons.go:231] Setting addon ingress=true in "minikube"
W0225 09:31:59.887320  745092 addons.go:240] addon ingress should already be in state true
I0225 09:31:59.887357  745092 host.go:66] Checking if "minikube" exists ...
I0225 09:31:59.887597  745092 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0225 09:31:59.887931  745092 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0225 09:31:59.887942  745092 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0225 09:31:59.897761  745092 out.go:177] 🔎  Verifying Kubernetes components...
I0225 09:31:59.898560  745092 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0225 09:31:59.911946  745092 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0225 09:31:59.912735  745092 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0225 09:31:59.912741  745092 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0225 09:31:59.912783  745092 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:31:59.913852  745092 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0225 09:31:59.913857  745092 addons.go:240] addon default-storageclass should already be in state true
I0225 09:31:59.913871  745092 host.go:66] Checking if "minikube" exists ...
I0225 09:31:59.914080  745092 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0225 09:31:59.941224  745092 out.go:177]     ▪ Using image registry.k8s.io/ingress-nginx/controller:v1.9.4
I0225 09:31:59.942054  745092 out.go:177]     ▪ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0
I0225 09:31:59.943010  745092 out.go:177]     ▪ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0
I0225 09:31:59.945589  745092 addons.go:423] installing /etc/kubernetes/addons/ingress-deploy.yaml
I0225 09:31:59.945606  745092 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/ingress-deploy.yaml (16103 bytes)
I0225 09:31:59.945659  745092 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:31:59.958388  745092 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32782 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0225 09:31:59.961973  745092 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0225 09:31:59.961983  745092 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0225 09:31:59.962020  745092 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:31:59.976130  745092 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32782 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0225 09:31:59.986283  745092 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32782 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0225 09:32:00.210480  745092 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0225 09:32:00.276580  745092 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0225 09:32:00.276607  745092 api_server.go:52] waiting for apiserver process to appear ...
I0225 09:32:00.276632  745092 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:32:00.282527  745092 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0225 09:32:00.359511  745092 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0225 09:32:01.428841  745092 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: (1.218345778s)
I0225 09:32:01.428854  745092 addons.go:467] Verifying addon ingress=true in "minikube"
I0225 09:32:01.429807  745092 out.go:177] 🔎  Verifying ingress addon...
I0225 09:32:01.431000  745092 kapi.go:75] Waiting for pod with label "app.kubernetes.io/name=ingress-nginx" in ns "ingress-nginx" ...
I0225 09:32:01.428922  745092 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.15227959s)
I0225 09:32:01.431084  745092 api_server.go:72] duration metric: took 1.543129202s to wait for apiserver process to appear ...
I0225 09:32:01.431088  745092 api_server.go:88] waiting for apiserver healthz status ...
I0225 09:32:01.431095  745092 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0225 09:32:01.428945  745092 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.146412017s)
I0225 09:32:01.429083  745092 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.069564184s)
I0225 09:32:01.434022  745092 kapi.go:86] Found 3 Pods for label selector app.kubernetes.io/name=ingress-nginx
I0225 09:32:01.434027  745092 kapi.go:107] duration metric: took 3.029734ms to wait for app.kubernetes.io/name=ingress-nginx ...
I0225 09:32:01.434333  745092 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0225 09:32:01.436114  745092 api_server.go:141] control plane version: v1.28.3
I0225 09:32:01.436119  745092 api_server.go:131] duration metric: took 5.028304ms to wait for apiserver health ...
I0225 09:32:01.436122  745092 system_pods.go:43] waiting for kube-system pods to appear ...
I0225 09:32:01.439600  745092 system_pods.go:59] 7 kube-system pods found
I0225 09:32:01.439607  745092 system_pods.go:61] "coredns-5dd5756b68-hdxcg" [3476c5b5-72c2-4358-aa08-c9d6f7c7ba94] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0225 09:32:01.439611  745092 system_pods.go:61] "etcd-minikube" [92e58708-2606-4d06-bb7e-d6122eca186e] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0225 09:32:01.439615  745092 system_pods.go:61] "kube-apiserver-minikube" [e20a0755-f590-40a9-b9b9-3bfb18d67518] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0225 09:32:01.439618  745092 system_pods.go:61] "kube-controller-manager-minikube" [87617930-059e-4547-a83a-8e6e26322c25] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0225 09:32:01.439621  745092 system_pods.go:61] "kube-proxy-cfj6z" [5d8bb303-bddf-441c-91b5-796fff7af79d] Running
I0225 09:32:01.439624  745092 system_pods.go:61] "kube-scheduler-minikube" [5bfbbdb4-39bc-4799-8549-354a523f358b] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0225 09:32:01.439626  745092 system_pods.go:61] "storage-provisioner" [4f4796b2-7736-451c-8980-4101c03710de] Running
I0225 09:32:01.439629  745092 system_pods.go:74] duration metric: took 3.504268ms to wait for pod list to return data ...
I0225 09:32:01.439633  745092 kubeadm.go:581] duration metric: took 1.551680353s to wait for : map[apiserver:true system_pods:true] ...
I0225 09:32:01.439639  745092 node_conditions.go:102] verifying NodePressure condition ...
I0225 09:32:01.440741  745092 out.go:177] 🌟  Enabled addons: storage-provisioner, ingress, default-storageclass
I0225 09:32:01.441610  745092 addons.go:502] enable addons completed in 1.561376922s: enabled=[storage-provisioner ingress default-storageclass]
I0225 09:32:01.442507  745092 node_conditions.go:122] node storage ephemeral capacity is 39019864Ki
I0225 09:32:01.442514  745092 node_conditions.go:123] node cpu capacity is 2
I0225 09:32:01.442519  745092 node_conditions.go:105] duration metric: took 2.877986ms to run NodePressure ...
I0225 09:32:01.442525  745092 start.go:228] waiting for startup goroutines ...
I0225 09:32:01.442528  745092 start.go:233] waiting for cluster config update ...
I0225 09:32:01.442533  745092 start.go:242] writing updated cluster config ...
I0225 09:32:01.442822  745092 ssh_runner.go:195] Run: rm -f paused
I0225 09:32:01.484535  745092 start.go:600] kubectl: 1.29.1, cluster: 1.28.3 (minor skew: 1)
I0225 09:32:01.485708  745092 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Feb 25 09:31:42 minikube dockerd[790]: time="2024-02-25T09:31:42.199045740Z" level=info msg="Docker daemon" commit=311b9ff graphdriver=overlay2 version=24.0.7
Feb 25 09:31:42 minikube dockerd[790]: time="2024-02-25T09:31:42.199205795Z" level=info msg="Daemon has completed initialization"
Feb 25 09:31:42 minikube dockerd[790]: time="2024-02-25T09:31:42.217113708Z" level=info msg="API listen on /var/run/docker.sock"
Feb 25 09:31:42 minikube systemd[1]: Started Docker Application Container Engine.
Feb 25 09:31:42 minikube dockerd[790]: time="2024-02-25T09:31:42.217565902Z" level=info msg="API listen on [::]:2376"
Feb 25 09:31:42 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Feb 25 09:31:42 minikube cri-dockerd[1000]: time="2024-02-25T09:31:42Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Feb 25 09:31:42 minikube cri-dockerd[1000]: time="2024-02-25T09:31:42Z" level=info msg="Start docker client with request timeout 0s"
Feb 25 09:31:42 minikube cri-dockerd[1000]: time="2024-02-25T09:31:42Z" level=info msg="Hairpin mode is set to hairpin-veth"
Feb 25 09:31:42 minikube cri-dockerd[1000]: time="2024-02-25T09:31:42Z" level=info msg="Loaded network plugin cni"
Feb 25 09:31:42 minikube cri-dockerd[1000]: time="2024-02-25T09:31:42Z" level=info msg="Docker cri networking managed by network plugin cni"
Feb 25 09:31:42 minikube cri-dockerd[1000]: time="2024-02-25T09:31:42Z" level=info msg="Docker Info: &{ID:c187eb58-e045-4926-96b8-668ba55ef355 Containers:50 ContainersRunning:0 ContainersPaused:0 ContainersStopped:50 Images:12 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:false NGoroutines:35 SystemTime:2024-02-25T09:31:42.65484653Z LoggingDriver:json-file CgroupDriver:systemd CgroupVersion:2 NEventsListener:0 KernelVersion:6.1.0-13-amd64 OperatingSystem:Ubuntu 22.04.3 LTS OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc000218e70 NCPU:2 MemTotal:6064132096 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523 Expected:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523} RuncCommit:{ID:v1.1.9-0-gccaecfc Expected:v1.1.9-0-gccaecfc} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: DefaultAddressPools:[] Warnings:[]}"
Feb 25 09:31:42 minikube cri-dockerd[1000]: time="2024-02-25T09:31:42Z" level=info msg="Setting cgroupDriver systemd"
Feb 25 09:31:42 minikube cri-dockerd[1000]: time="2024-02-25T09:31:42Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Feb 25 09:31:42 minikube cri-dockerd[1000]: time="2024-02-25T09:31:42Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Feb 25 09:31:42 minikube cri-dockerd[1000]: time="2024-02-25T09:31:42Z" level=info msg="Start cri-dockerd grpc backend"
Feb 25 09:31:42 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Feb 25 09:31:54 minikube cri-dockerd[1000]: time="2024-02-25T09:31:54Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-hdxcg_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"4d8ee3b7c477022cf9b332a7b44e9b2d7a427e27573abfb486f6e7ce56d152af\""
Feb 25 09:31:54 minikube cri-dockerd[1000]: time="2024-02-25T09:31:54Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-hdxcg_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"87afe8fa23d45f0769183cec15179e2b13121c281ffc7979eb385748372a3d2b\""
Feb 25 09:31:54 minikube cri-dockerd[1000]: time="2024-02-25T09:31:54Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-create-wm8b9_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"e611c74700c4ffbd53a67747d8b55c22762fdf12dd44d5271986b98df8476e37\""
Feb 25 09:31:54 minikube cri-dockerd[1000]: time="2024-02-25T09:31:54Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"web-deploy-5655fcd986-pklvd_homework\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6e6aeb1fa85cb999048c81c70e02a7ba01a7144d3820c6102353a71006f5ed81\""
Feb 25 09:31:54 minikube cri-dockerd[1000]: time="2024-02-25T09:31:54Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"web-deploy-5655fcd986-pklvd_homework\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"0d314424fe2cee8f6ae593b67871cf57f42b95ccea6da97714e5a8fb3f6ccb67\""
Feb 25 09:31:54 minikube cri-dockerd[1000]: time="2024-02-25T09:31:54Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"web-deploy-5655fcd986-kfqrq_homework\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"b3860dab0ae1154dc998063302814f5fff1fee811790bbc5796a95f4e94cc9e8\""
Feb 25 09:31:54 minikube cri-dockerd[1000]: time="2024-02-25T09:31:54Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"web-deploy-5655fcd986-kfqrq_homework\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"072fbd6ac717a184e65840d9564d9653c8e19accd04ce60f715dd9c4e9416bcb\""
Feb 25 09:31:54 minikube cri-dockerd[1000]: time="2024-02-25T09:31:54Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-controller-7c6974c4d8-8lfbr_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"dc8cf491203487b84b73bb5c86a9eb650224fcc1d489aaf98da099760e59a854\""
Feb 25 09:31:54 minikube cri-dockerd[1000]: time="2024-02-25T09:31:54Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-controller-7c6974c4d8-8lfbr_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"3dff6ba809ece9a9678138e5c5893280b4bf46b0e3356c8ade50dd19a842710d\""
Feb 25 09:31:54 minikube cri-dockerd[1000]: time="2024-02-25T09:31:54Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-patch-d99lv_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c903bc6261618fb856b336d40ffbd5ca5a7fad8841a9c529bb129152b28511e7\""
Feb 25 09:31:54 minikube cri-dockerd[1000]: time="2024-02-25T09:31:54Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"web-deploy-5655fcd986-6lctw_homework\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"f5c355693bd021dbeb75b4bfb0c391d035f4372979d333da4a1b9249cb31bb53\""
Feb 25 09:31:54 minikube cri-dockerd[1000]: time="2024-02-25T09:31:54Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"web-deploy-5655fcd986-6lctw_homework\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"320b981ae416968735970677f21d8da5b15d05ff4facd1d5bbb9af167bbeb852\""
Feb 25 09:31:55 minikube cri-dockerd[1000]: time="2024-02-25T09:31:55Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"c2f8f484c8d0a013f6bf94ce4f6ca1467581eb6aa96299301ec38489afaaa0cb\". Proceed without further sandbox information."
Feb 25 09:31:55 minikube cri-dockerd[1000]: time="2024-02-25T09:31:55Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"6d417a192731653a9b0b8d267468ea8cb6e5415df1c18133a117515a6db82f69\". Proceed without further sandbox information."
Feb 25 09:31:55 minikube cri-dockerd[1000]: time="2024-02-25T09:31:55Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"a4b95480b009dd5e73261df856e117b813da5f15bbc4f47cadf23aaedde6ecab\". Proceed without further sandbox information."
Feb 25 09:31:55 minikube cri-dockerd[1000]: time="2024-02-25T09:31:55Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"7205e6dab76268a9ca1b2353b28b0a4b3043f2cb433711420e71c6549bb4e47a\". Proceed without further sandbox information."
Feb 25 09:31:55 minikube cri-dockerd[1000]: time="2024-02-25T09:31:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0eff2a2274e4875a52bcb5ed825732edc5521240edbd8ff949980a444eb42016/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Feb 25 09:31:55 minikube cri-dockerd[1000]: time="2024-02-25T09:31:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/51af094fe0770e0af6ebbf417736be52104026409bb5c4dd4f0fe0c36396248c/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Feb 25 09:31:55 minikube cri-dockerd[1000]: time="2024-02-25T09:31:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7b36039ad918ff36b702beb3f80feba4b9a50466d51e93effa42469344b56951/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Feb 25 09:31:55 minikube cri-dockerd[1000]: time="2024-02-25T09:31:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2364ddfc14298241c78e31faa28e164c1ab8ab53e21f2e36510f415798d63c51/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Feb 25 09:31:55 minikube cri-dockerd[1000]: time="2024-02-25T09:31:55Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"web-deploy-5655fcd986-pklvd_homework\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6e6aeb1fa85cb999048c81c70e02a7ba01a7144d3820c6102353a71006f5ed81\""
Feb 25 09:31:55 minikube cri-dockerd[1000]: time="2024-02-25T09:31:55Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"web-deploy-5655fcd986-kfqrq_homework\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"b3860dab0ae1154dc998063302814f5fff1fee811790bbc5796a95f4e94cc9e8\""
Feb 25 09:31:55 minikube cri-dockerd[1000]: time="2024-02-25T09:31:55Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-controller-7c6974c4d8-8lfbr_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"dc8cf491203487b84b73bb5c86a9eb650224fcc1d489aaf98da099760e59a854\""
Feb 25 09:31:55 minikube cri-dockerd[1000]: time="2024-02-25T09:31:55Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"web-deploy-5655fcd986-6lctw_homework\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"f5c355693bd021dbeb75b4bfb0c391d035f4372979d333da4a1b9249cb31bb53\""
Feb 25 09:31:55 minikube cri-dockerd[1000]: time="2024-02-25T09:31:55Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-hdxcg_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"4d8ee3b7c477022cf9b332a7b44e9b2d7a427e27573abfb486f6e7ce56d152af\""
Feb 25 09:31:58 minikube cri-dockerd[1000]: time="2024-02-25T09:31:58Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Feb 25 09:31:59 minikube cri-dockerd[1000]: time="2024-02-25T09:31:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1a9d3e99ad1fd6fd3f1edf7b80ff769476d4bb3ebf3bfd7598a84b4323f45fa4/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Feb 25 09:31:59 minikube cri-dockerd[1000]: time="2024-02-25T09:31:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/08e02766d76cf0651423636ce268c0aa2a7bef0cb2dc063a72a47e80c4dffa56/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Feb 25 09:31:59 minikube cri-dockerd[1000]: time="2024-02-25T09:31:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5a334e95ead05204b68667352c20553fa091847f31cb2df915f13599055a3208/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Feb 25 09:31:59 minikube cri-dockerd[1000]: time="2024-02-25T09:31:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4826a96363add51fb580a7c7ee9c2f1f4ac4d53cdf169050e055117694c627f3/resolv.conf as [nameserver 10.96.0.10 search homework.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 25 09:31:59 minikube cri-dockerd[1000]: time="2024-02-25T09:31:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5163d2d558343d3021c2e331849f86660b8cf977a0d6edb41af851d338a8c460/resolv.conf as [nameserver 10.96.0.10 search homework.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 25 09:32:00 minikube cri-dockerd[1000]: time="2024-02-25T09:32:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ca9f1d3588698852feefe5b1fbbe6b0008d58dbe41987f5fe553c00a74e13f90/resolv.conf as [nameserver 10.96.0.10 search homework.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 25 09:32:00 minikube cri-dockerd[1000]: time="2024-02-25T09:32:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/73d25e583cef85677d9f5bcf39ef735511ea66d9ee97bd6fe1a43950f8aae088/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 25 09:32:01 minikube cri-dockerd[1000]: time="2024-02-25T09:32:01Z" level=info msg="Stop pulling image curlimages/curl:latest: Status: Image is up to date for curlimages/curl:latest"
Feb 25 09:32:01 minikube dockerd[790]: time="2024-02-25T09:32:01.337966543Z" level=info msg="ignoring event" container=426f02d302b2a5e136e9f1fea4f78d999605713b79f1841b9e1d2858d8f5d7f5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 25 09:32:02 minikube cri-dockerd[1000]: time="2024-02-25T09:32:02Z" level=info msg="Stop pulling image curlimages/curl:latest: Status: Image is up to date for curlimages/curl:latest"
Feb 25 09:32:02 minikube dockerd[790]: time="2024-02-25T09:32:02.582105436Z" level=info msg="ignoring event" container=329e92bf9b50467d587e21159e901748c65545935dbae76f9b735f8617c9c8fa module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 25 09:32:03 minikube cri-dockerd[1000]: time="2024-02-25T09:32:03Z" level=info msg="Stop pulling image curlimages/curl:latest: Status: Image is up to date for curlimages/curl:latest"
Feb 25 09:32:03 minikube dockerd[790]: time="2024-02-25T09:32:03.881410591Z" level=info msg="ignoring event" container=fc55ce0f74c8d08e988e213e82b72fb637b6bf0d09e36692e5469826a29831d7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 25 09:32:05 minikube cri-dockerd[1000]: time="2024-02-25T09:32:05Z" level=info msg="Stop pulling image tulamelki/neweng:latest: Status: Image is up to date for tulamelki/neweng:latest"
Feb 25 09:32:06 minikube cri-dockerd[1000]: time="2024-02-25T09:32:06Z" level=info msg="Stop pulling image tulamelki/neweng:latest: Status: Image is up to date for tulamelki/neweng:latest"
Feb 25 09:32:07 minikube cri-dockerd[1000]: time="2024-02-25T09:32:07Z" level=info msg="Stop pulling image tulamelki/neweng:latest: Status: Image is up to date for tulamelki/neweng:latest"
Feb 25 09:32:29 minikube dockerd[790]: time="2024-02-25T09:32:29.634003710Z" level=info msg="ignoring event" container=e4e5f661a4f9adbfdd63b0417a8fc61b77df0ae8f2868da79dec682021c26c80 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
81cb2af86bb44       tulamelki/neweng@sha256:aa6babc55e0ff74793b821cdef9039b40dfb77adc0767f1daac2ea1aaf85bf47                                     37 seconds ago      Running             webnginx                  2                   ca9f1d3588698       web-deploy-5655fcd986-pklvd
f7b7df50546dd       tulamelki/neweng@sha256:aa6babc55e0ff74793b821cdef9039b40dfb77adc0767f1daac2ea1aaf85bf47                                     38 seconds ago      Running             webnginx                  2                   5163d2d558343       web-deploy-5655fcd986-kfqrq
0fdd1ef3a284a       tulamelki/neweng@sha256:aa6babc55e0ff74793b821cdef9039b40dfb77adc0767f1daac2ea1aaf85bf47                                     39 seconds ago      Running             webnginx                  2                   4826a96363add       web-deploy-5655fcd986-6lctw
fc55ce0f74c8d       curlimages/curl@sha256:c3b8bee303c6c6beed656cfc921218c529d65aa61114eb9e27c62047a1271b9b                                      41 seconds ago      Exited              init-containers           2                   ca9f1d3588698       web-deploy-5655fcd986-pklvd
329e92bf9b504       curlimages/curl@sha256:c3b8bee303c6c6beed656cfc921218c529d65aa61114eb9e27c62047a1271b9b                                      42 seconds ago      Exited              init-containers           2                   5163d2d558343       web-deploy-5655fcd986-kfqrq
426f02d302b2a       curlimages/curl@sha256:c3b8bee303c6c6beed656cfc921218c529d65aa61114eb9e27c62047a1271b9b                                      43 seconds ago      Exited              init-containers           2                   4826a96363add       web-deploy-5655fcd986-6lctw
25f2c8bc9d620       5aa0bf4798fa2                                                                                                                44 seconds ago      Running             controller                2                   73d25e583cef8       ingress-nginx-controller-7c6974c4d8-8lfbr
0e4822af5bf24       ead0a4a53df89                                                                                                                45 seconds ago      Running             coredns                   5                   5a334e95ead05       coredns-5dd5756b68-hdxcg
e4e5f661a4f9a       6e38f40d628db                                                                                                                45 seconds ago      Exited              storage-provisioner       9                   08e02766d76cf       storage-provisioner
bc995854f0761       bfc896cf80fba                                                                                                                45 seconds ago      Running             kube-proxy                5                   1a9d3e99ad1fd       kube-proxy-cfj6z
8d8b5a8d40fd2       6d1b4fd1b182d                                                                                                                49 seconds ago      Running             kube-scheduler            5                   2364ddfc14298       kube-scheduler-minikube
2e50018409013       5374347291230                                                                                                                49 seconds ago      Running             kube-apiserver            5                   7b36039ad918f       kube-apiserver-minikube
e6cb7417e651d       73deb9a3f7025                                                                                                                49 seconds ago      Running             etcd                      5                   51af094fe0770       etcd-minikube
8cd9e6087dab2       10baa1ca17068                                                                                                                49 seconds ago      Running             kube-controller-manager   5                   0eff2a2274e48       kube-controller-manager-minikube
2d9d8fee0d876       tulamelki/neweng@sha256:aa6babc55e0ff74793b821cdef9039b40dfb77adc0767f1daac2ea1aaf85bf47                                     9 minutes ago       Exited              webnginx                  1                   b3860dab0ae11       web-deploy-5655fcd986-kfqrq
7aa72602f1be5       tulamelki/neweng@sha256:aa6babc55e0ff74793b821cdef9039b40dfb77adc0767f1daac2ea1aaf85bf47                                     9 minutes ago       Exited              webnginx                  1                   f5c355693bd02       web-deploy-5655fcd986-6lctw
7ed6fd081b516       tulamelki/neweng@sha256:aa6babc55e0ff74793b821cdef9039b40dfb77adc0767f1daac2ea1aaf85bf47                                     9 minutes ago       Exited              webnginx                  1                   6e6aeb1fa85cb       web-deploy-5655fcd986-pklvd
cdd0de90435e3       5aa0bf4798fa2                                                                                                                9 minutes ago       Exited              controller                1                   dc8cf49120348       ingress-nginx-controller-7c6974c4d8-8lfbr
0baf065f27c56       ead0a4a53df89                                                                                                                9 minutes ago       Exited              coredns                   4                   4d8ee3b7c4770       coredns-5dd5756b68-hdxcg
5e706d51b2c26       bfc896cf80fba                                                                                                                9 minutes ago       Exited              kube-proxy                4                   8987011dea02b       kube-proxy-cfj6z
67700306ce168       6d1b4fd1b182d                                                                                                                9 minutes ago       Exited              kube-scheduler            4                   4b81ae6c33696       kube-scheduler-minikube
4a616b73cec74       10baa1ca17068                                                                                                                9 minutes ago       Exited              kube-controller-manager   4                   81dac51431a76       kube-controller-manager-minikube
372b1bb82a5a6       5374347291230                                                                                                                9 minutes ago       Exited              kube-apiserver            4                   36257e0c48a10       kube-apiserver-minikube
fbebf49eaa922       73deb9a3f7025                                                                                                                9 minutes ago       Exited              etcd                      4                   44ae95d353f22       etcd-minikube
7664470e7dc59       1ebff0f9671bc                                                                                                                About an hour ago   Exited              patch                     1                   c903bc6261618       ingress-nginx-admission-patch-d99lv
e437756fbc4ae       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80   About an hour ago   Exited              create                    0                   e611c74700c4f       ingress-nginx-admission-create-wm8b9

* 
* ==> controller_ingress [25f2c8bc9d62] <==
* W0225 09:32:00.548884       7 client_config.go:618] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0225 09:32:00.548976       7 main.go:205] "Creating API client" host="https://10.96.0.1:443"
I0225 09:32:00.556188       7 main.go:249] "Running in Kubernetes cluster" major="1" minor="28" git="v1.28.3" state="clean" commit="a8a1abc25cad87333840cd7d54be2efaf31a3177" platform="linux/amd64"
I0225 09:32:00.806305       7 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0225 09:32:00.826767       7 ssl.go:536] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0225 09:32:00.840369       7 nginx.go:260] "Starting NGINX Ingress controller"
I0225 09:32:00.853022       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"9f848854-40ca-4528-b121-16d5780f184b", APIVersion:"v1", ResourceVersion:"68771", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0225 09:32:00.861796       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"e4f5857d-4b00-441e-b65e-6057a0016ec6", APIVersion:"v1", ResourceVersion:"68772", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0225 09:32:00.861857       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"af032e48-f5dd-4fb7-b64a-4a27c7362898", APIVersion:"v1", ResourceVersion:"68773", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0225 09:32:01.956160       7 store.go:440] "Found valid IngressClass" ingress="homework/ingress-host" ingressclass="_"
I0225 09:32:01.958745       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"homework", Name:"ingress-host", UID:"f1c9b16d-f20d-4d42-9c70-c7ae570f71b7", APIVersion:"networking.k8s.io/v1", ResourceVersion:"71987", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0225 09:32:02.050693       7 nginx.go:303] "Starting NGINX process"
I0225 09:32:02.050734       7 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0225 09:32:02.052691       7 nginx.go:323] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0225 09:32:02.056376       7 controller.go:190] "Configuration changes detected, backend reload required"
I0225 09:32:02.078944       7 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0225 09:32:02.079389       7 status.go:84] "New leader elected" identity="ingress-nginx-controller-7c6974c4d8-8lfbr"
I0225 09:32:02.089260       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-7c6974c4d8-8lfbr" node="minikube"
I0225 09:32:02.091922       7 status.go:304] "updating Ingress status" namespace="homework" ingress="ingress-host" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I0225 09:32:02.097917       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"homework", Name:"ingress-host", UID:"f1c9b16d-f20d-4d42-9c70-c7ae570f71b7", APIVersion:"networking.k8s.io/v1", ResourceVersion:"72495", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0225 09:32:02.140454       7 controller.go:210] "Backend successfully reloaded"
I0225 09:32:02.140513       7 controller.go:221] "Initial sync, sleeping for 1 second"
I0225 09:32:02.140831       7 event.go:298] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-7c6974c4d8-8lfbr", UID:"25399ab7-40df-48a9-8b42-4c78d0d05e2e", APIVersion:"v1", ResourceVersion:"72404", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.9.4
  Build:         846d251814a09d8a5d8d28e2e604bfc7749bcb49
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.21.6

-------------------------------------------------------------------------------

192.168.49.1 - - [25/Feb/2024:09:32:14 +0000] "GET / HTTP/1.1" 403 153 "-" "curl/7.88.1" 77 0.002 [homework-server-nginx-80] [] 10.244.0.114:8000 153 0.002 403 36a1e0f78ed6f05c069bb9a575ceb6d9
192.168.49.1 - - [25/Feb/2024:09:32:20 +0000] "GET /metrics HTTP/1.1" 404 153 "-" "curl/7.88.1" 84 0.002 [homework-server-nginx-80] [] 10.244.0.112:8000 153 0.002 404 4aac20c6b9f8041267484ab5cae429ce

* 
* ==> controller_ingress [cdd0de90435e] <==
* -------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.9.4
  Build:         846d251814a09d8a5d8d28e2e604bfc7749bcb49
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.21.6

-------------------------------------------------------------------------------

192.168.49.1 - - [25/Feb/2024:09:25:46 +0000] "GET / HTTP/1.1" 403 153 "-" "curl/7.88.1" 77 0.002 [homework-server-nginx-80] [] 10.244.0.108:8000 153 0.003 403 7ca5514edfe12895f4ac996672430140
W0225 09:22:58.916022       7 client_config.go:618] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0225 09:22:58.916181       7 main.go:205] "Creating API client" host="https://10.96.0.1:443"
I0225 09:22:58.940763       7 main.go:249] "Running in Kubernetes cluster" major="1" minor="28" git="v1.28.3" state="clean" commit="a8a1abc25cad87333840cd7d54be2efaf31a3177" platform="linux/amd64"
I0225 09:22:59.075543       7 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0225 09:22:59.166488       7 ssl.go:536] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0225 09:22:59.183880       7 nginx.go:260] "Starting NGINX Ingress controller"
I0225 09:22:59.203282       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"9f848854-40ca-4528-b121-16d5780f184b", APIVersion:"v1", ResourceVersion:"68771", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0225 09:22:59.203525       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"e4f5857d-4b00-441e-b65e-6057a0016ec6", APIVersion:"v1", ResourceVersion:"68772", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0225 09:22:59.203591       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"af032e48-f5dd-4fb7-b64a-4a27c7362898", APIVersion:"v1", ResourceVersion:"68773", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0225 09:23:00.290477       7 store.go:440] "Found valid IngressClass" ingress="homework/ingress-host" ingressclass="_"
I0225 09:23:00.292889       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"homework", Name:"ingress-host", UID:"f1c9b16d-f20d-4d42-9c70-c7ae570f71b7", APIVersion:"networking.k8s.io/v1", ResourceVersion:"68945", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0225 09:23:00.385897       7 nginx.go:303] "Starting NGINX process"
I0225 09:23:00.386877       7 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0225 09:23:00.387117       7 nginx.go:323] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0225 09:23:00.391006       7 controller.go:190] "Configuration changes detected, backend reload required"
I0225 09:23:00.423548       7 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0225 09:23:00.423765       7 status.go:84] "New leader elected" identity="ingress-nginx-controller-7c6974c4d8-8lfbr"
I0225 09:23:00.427289       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-7c6974c4d8-8lfbr" node="minikube"
I0225 09:23:00.432156       7 status.go:304] "updating Ingress status" namespace="homework" ingress="ingress-host" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I0225 09:23:00.441458       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"homework", Name:"ingress-host", UID:"f1c9b16d-f20d-4d42-9c70-c7ae570f71b7", APIVersion:"networking.k8s.io/v1", ResourceVersion:"71828", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0225 09:23:00.477080       7 controller.go:210] "Backend successfully reloaded"
I0225 09:23:00.477137       7 controller.go:221] "Initial sync, sleeping for 1 second"
I0225 09:23:00.477160       7 event.go:298] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-7c6974c4d8-8lfbr", UID:"25399ab7-40df-48a9-8b42-4c78d0d05e2e", APIVersion:"v1", ResourceVersion:"71808", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
I0225 09:24:00.453448       7 status.go:304] "updating Ingress status" namespace="homework" ingress="ingress-host" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I0225 09:24:00.459077       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"homework", Name:"ingress-host", UID:"f1c9b16d-f20d-4d42-9c70-c7ae570f71b7", APIVersion:"networking.k8s.io/v1", ResourceVersion:"71987", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0225 09:31:22.117338       7 sigterm.go:36] "Received SIGTERM, shutting down"
I0225 09:31:22.117362       7 nginx.go:379] "Shutting down controller queues"
E0225 09:31:22.117884       7 status.go:120] "error obtaining running IP address" err="Get \"https://10.96.0.1:443/api/v1/namespaces/ingress-nginx/pods?labelSelector=app.kubernetes.io%!F(MISSING)component%!D(MISSING)controller%!C(MISSING)app.kubernetes.io%!F(MISSING)instance%!D(MISSING)ingress-nginx%!C(MISSING)app.kubernetes.io%!F(MISSING)name%!D(MISSING)ingress-nginx%!C(MISSING)gcp-auth-skip-secret%!D(MISSING)true%!C(MISSING)pod-template-hash%!D(MISSING)7c6974c4d8\": dial tcp 10.96.0.1:443: connect: connection refused"
I0225 09:31:22.117892       7 nginx.go:387] "Stopping admission controller"
E0225 09:31:22.117911       7 nginx.go:326] "Error listening for TLS connections" err="http: Server closed"
I0225 09:31:22.117915       7 nginx.go:395] "Stopping NGINX process"
2024/02/25 09:31:22 [notice] 94#94: signal process started
I0225 09:31:23.157259       7 nginx.go:408] "NGINX process has stopped"
I0225 09:31:23.157275       7 sigterm.go:44] Handled quit, delaying controller exit for 10 seconds
E0225 09:31:24.075716       7 leaderelection.go:327] error retrieving resource lock ingress-nginx/ingress-nginx-leader: Get "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader": dial tcp 10.96.0.1:443: connect: connection refused
E0225 09:31:31.577428       7 leaderelection.go:327] error retrieving resource lock ingress-nginx/ingress-nginx-leader: Get "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader": dial tcp 10.96.0.1:443: connect: connection refused

* 
* ==> coredns [0baf065f27c5] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:32965 - 50833 "HINFO IN 2625473490364411110.5906982061204743098. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.048050416s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [0e4822af5bf2] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:53705 - 15475 "HINFO IN 5530746496487375506.5333939080968495981. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.109376363s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_02_24T10_18_10_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 24 Feb 2024 10:18:08 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 25 Feb 2024 09:32:38 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 25 Feb 2024 09:31:58 +0000   Sat, 24 Feb 2024 10:18:06 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 25 Feb 2024 09:31:58 +0000   Sat, 24 Feb 2024 10:18:06 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 25 Feb 2024 09:31:58 +0000   Sat, 24 Feb 2024 10:18:06 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 25 Feb 2024 09:31:58 +0000   Sat, 24 Feb 2024 10:18:08 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  39019864Ki
  hugepages-2Mi:      0
  memory:             5922004Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  39019864Ki
  hugepages-2Mi:      0
  memory:             5922004Ki
  pods:               110
System Info:
  Machine ID:                 1b707c47027e4eeab3c8ab478a9d9bcb
  System UUID:                15151353-21c3-47aa-a14b-c414559bcbe2
  Boot ID:                    4e41f121-8dd4-4180-9786-bf26d1857075
  Kernel Version:             6.1.0-13-amd64
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  homework                    web-deploy-5655fcd986-6lctw                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         32m
  homework                    web-deploy-5655fcd986-kfqrq                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         32m
  homework                    web-deploy-5655fcd986-pklvd                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         32m
  ingress-nginx               ingress-nginx-controller-7c6974c4d8-8lfbr    100m (5%!)(MISSING)     0 (0%!)(MISSING)      90Mi (1%!)(MISSING)        0 (0%!)(MISSING)         61m
  kube-system                 coredns-5dd5756b68-hdxcg                     100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (2%!)(MISSING)     23h
  kube-system                 etcd-minikube                                100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         23h
  kube-system                 kube-apiserver-minikube                      250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23h
  kube-system                 kube-controller-manager-minikube             200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23h
  kube-system                 kube-proxy-cfj6z                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23h
  kube-system                 kube-scheduler-minikube                      100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23h
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (42%!)(MISSING)  0 (0%!)(MISSING)
  memory             260Mi (4%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 9m46s                  kube-proxy       
  Normal  Starting                 44s                    kube-proxy       
  Normal  NodeAllocatableEnforced  9m51s                  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  9m51s (x8 over 9m51s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    9m51s (x8 over 9m51s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     9m51s (x7 over 9m51s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 9m51s                  kubelet          Starting kubelet.
  Normal  RegisteredNode           9m35s                  node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 50s                    kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  50s (x8 over 50s)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    50s (x8 over 50s)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     50s (x7 over 50s)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  50s                    kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           34s                    node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Feb24 23:03] RETBleed: WARNING: Spectre v2 mitigation leaves CPU vulnerable to RETBleed attacks, data leaks possible!
[  +0.144210] acpi PNP0A03:00: fail to add MMCONFIG information, can't access extended PCI configuration space under this bridge.
[  +2.105207] systemd[1]: Invalid DMI field header.
[  +0.269693] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.345035] vboxvideo 0000:00:02.0: [drm] drm_plane_enable_fb_damage_clips() not called

* 
* ==> etcd [e6cb7417e651] <==
* {"level":"warn","ts":"2024-02-25T09:31:55.427096Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-02-25T09:31:55.427152Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-02-25T09:31:55.427204Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-02-25T09:31:55.427219Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-02-25T09:31:55.427229Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-02-25T09:31:55.427248Z","caller":"embed/etcd.go:495","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-02-25T09:31:55.42748Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-02-25T09:31:55.427558Z","caller":"embed/etcd.go:309","msg":"starting an etcd server","etcd-version":"3.5.9","git-sha":"bdbbde998","go-version":"go1.19.9","go-os":"linux","go-arch":"amd64","max-cpu-set":2,"max-cpu-available":2,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-02-25T09:31:55.428831Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"1.149453ms"}
{"level":"info","ts":"2024-02-25T09:31:56.408696Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":80008,"snapshot-size":"8.2 kB"}
{"level":"info","ts":"2024-02-25T09:31:56.408733Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":4096000,"backend-size":"4.1 MB","backend-size-in-use-bytes":2867200,"backend-size-in-use":"2.9 MB"}
{"level":"info","ts":"2024-02-25T09:31:56.529158Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":89566}
{"level":"info","ts":"2024-02-25T09:31:56.529437Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-02-25T09:31:56.529462Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 6"}
{"level":"info","ts":"2024-02-25T09:31:56.529524Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 6, commit: 89566, applied: 80008, lastindex: 89566, lastterm: 6]"}
{"level":"info","ts":"2024-02-25T09:31:56.529621Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-02-25T09:31:56.529634Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-02-25T09:31:56.529641Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-02-25T09:31:56.530344Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-02-25T09:31:56.53093Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":71332}
{"level":"info","ts":"2024-02-25T09:31:56.536126Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":72397}
{"level":"info","ts":"2024-02-25T09:31:56.553261Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-02-25T09:31:56.559979Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-02-25T09:31:56.560358Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-02-25T09:31:56.560385Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.9","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-02-25T09:31:56.561458Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-02-25T09:31:56.561602Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-02-25T09:31:56.561621Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-02-25T09:31:56.561683Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-02-25T09:31:56.561768Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-25T09:31:56.561786Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-25T09:31:56.561792Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-25T09:31:56.561906Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-25T09:31:56.561917Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-25T09:31:57.130422Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 6"}
{"level":"info","ts":"2024-02-25T09:31:57.130504Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 6"}
{"level":"info","ts":"2024-02-25T09:31:57.130548Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 6"}
{"level":"info","ts":"2024-02-25T09:31:57.130583Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 7"}
{"level":"info","ts":"2024-02-25T09:31:57.130593Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 7"}
{"level":"info","ts":"2024-02-25T09:31:57.130605Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 7"}
{"level":"info","ts":"2024-02-25T09:31:57.130614Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 7"}
{"level":"info","ts":"2024-02-25T09:31:57.132249Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-02-25T09:31:57.132527Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-02-25T09:31:57.132693Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-02-25T09:31:57.13275Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-02-25T09:31:57.132547Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-02-25T09:31:57.133843Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-02-25T09:31:57.134312Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}

* 
* ==> etcd [fbebf49eaa92] <==
* {"level":"warn","ts":"2024-02-25T09:22:54.045927Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-02-25T09:22:54.04613Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-02-25T09:22:54.046174Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-02-25T09:22:54.046191Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-02-25T09:22:54.046196Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-02-25T09:22:54.046216Z","caller":"embed/etcd.go:495","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-02-25T09:22:54.046484Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-02-25T09:22:54.046598Z","caller":"embed/etcd.go:309","msg":"starting an etcd server","etcd-version":"3.5.9","git-sha":"bdbbde998","go-version":"go1.19.9","go-os":"linux","go-arch":"amd64","max-cpu-set":2,"max-cpu-available":2,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-02-25T09:22:54.056288Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"9.535224ms"}
{"level":"info","ts":"2024-02-25T09:22:55.038956Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":80008,"snapshot-size":"8.2 kB"}
{"level":"info","ts":"2024-02-25T09:22:55.039255Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":4096000,"backend-size":"4.1 MB","backend-size-in-use-bytes":1642496,"backend-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-02-25T09:22:55.107287Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":88846}
{"level":"info","ts":"2024-02-25T09:22:55.107918Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-02-25T09:22:55.107981Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 5"}
{"level":"info","ts":"2024-02-25T09:22:55.108008Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 5, commit: 88846, applied: 80008, lastindex: 88846, lastterm: 5]"}
{"level":"info","ts":"2024-02-25T09:22:55.108102Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-02-25T09:22:55.108133Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-02-25T09:22:55.108156Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-02-25T09:22:55.109224Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-02-25T09:22:55.109696Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":71332}
{"level":"info","ts":"2024-02-25T09:22:55.111285Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":71797}
{"level":"info","ts":"2024-02-25T09:22:55.113512Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-02-25T09:22:55.114348Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-02-25T09:22:55.114629Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-02-25T09:22:55.114694Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.9","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-02-25T09:22:55.114808Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-02-25T09:22:55.115928Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-02-25T09:22:55.1269Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-02-25T09:22:55.127316Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-02-25T09:22:55.119368Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-25T09:22:55.131994Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-25T09:22:55.119321Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-25T09:22:55.137036Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-25T09:22:55.13706Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-25T09:22:55.308817Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 5"}
{"level":"info","ts":"2024-02-25T09:22:55.308909Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 5"}
{"level":"info","ts":"2024-02-25T09:22:55.30895Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2024-02-25T09:22:55.308973Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 6"}
{"level":"info","ts":"2024-02-25T09:22:55.308993Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 6"}
{"level":"info","ts":"2024-02-25T09:22:55.30901Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 6"}
{"level":"info","ts":"2024-02-25T09:22:55.309029Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 6"}
{"level":"info","ts":"2024-02-25T09:22:55.320559Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-02-25T09:22:55.320607Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-02-25T09:22:55.321196Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-02-25T09:22:55.321337Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-02-25T09:22:55.321813Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-02-25T09:22:55.334271Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-02-25T09:22:55.334309Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-02-25T09:31:22.051576Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-02-25T09:31:22.051613Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-02-25T09:31:22.051654Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-02-25T09:31:22.051704Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-02-25T09:31:22.11577Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-02-25T09:31:22.115805Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-02-25T09:31:22.115837Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-02-25T09:31:22.119844Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-25T09:31:22.119941Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-25T09:31:22.11995Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  09:32:44 up 10:29,  0 users,  load average: 2.99, 1.58, 1.02
Linux minikube 6.1.0-13-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.55-1 (2023-09-29) x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [2e5001840901] <==
* I0225 09:31:57.549822       1 handler.go:232] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0225 09:31:57.549835       1 genericapiserver.go:744] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0225 09:31:57.579961       1 handler.go:232] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0225 09:31:57.579977       1 genericapiserver.go:744] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0225 09:31:57.914221       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0225 09:31:57.914411       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0225 09:31:57.914568       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0225 09:31:57.915161       1 secure_serving.go:213] Serving securely on [::]:8443
I0225 09:31:57.915190       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0225 09:31:57.915271       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0225 09:31:57.915324       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0225 09:31:57.915377       1 aggregator.go:164] waiting for initial CRD sync...
I0225 09:31:57.915447       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0225 09:31:57.915499       1 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0225 09:31:57.915696       1 controller.go:116] Starting legacy_token_tracking_controller
I0225 09:31:57.915758       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0225 09:31:57.915829       1 apf_controller.go:372] Starting API Priority and Fairness config controller
I0225 09:31:57.916553       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0225 09:31:57.916649       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0225 09:31:57.916719       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0225 09:31:57.916770       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0225 09:31:57.917043       1 controller.go:134] Starting OpenAPI controller
I0225 09:31:57.922160       1 controller.go:85] Starting OpenAPI V3 controller
I0225 09:31:57.922252       1 naming_controller.go:291] Starting NamingConditionController
I0225 09:31:57.922314       1 establishing_controller.go:76] Starting EstablishingController
I0225 09:31:57.924391       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0225 09:31:57.924405       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0225 09:31:57.924418       1 crd_finalizer.go:266] Starting CRDFinalizer
I0225 09:31:57.917080       1 controller.go:78] Starting OpenAPI AggregationController
I0225 09:31:57.917282       1 available_controller.go:423] Starting AvailableConditionController
I0225 09:31:57.925018       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0225 09:31:57.917360       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0225 09:31:57.925120       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0225 09:31:57.917435       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0225 09:31:57.917494       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0225 09:31:57.917523       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0225 09:31:57.942489       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0225 09:31:57.942637       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0225 09:31:57.993415       1 shared_informer.go:318] Caches are synced for node_authorizer
I0225 09:31:57.997295       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0225 09:31:58.015443       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0225 09:31:58.018466       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0225 09:31:58.022417       1 shared_informer.go:318] Caches are synced for configmaps
I0225 09:31:58.022710       1 apf_controller.go:377] Running API Priority and Fairness config worker
I0225 09:31:58.022721       1 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
I0225 09:31:58.022850       1 aggregator.go:166] initial CRD sync complete...
I0225 09:31:58.022860       1 autoregister_controller.go:141] Starting autoregister controller
I0225 09:31:58.022863       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0225 09:31:58.022867       1 cache.go:39] Caches are synced for autoregister controller
I0225 09:31:58.025671       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0225 09:31:58.025874       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
E0225 09:31:58.028659       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0225 09:31:58.922892       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0225 09:31:59.713035       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0225 09:31:59.724266       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0225 09:31:59.774880       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0225 09:31:59.843605       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0225 09:31:59.856011       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0225 09:32:10.265799       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0225 09:32:10.525275       1 controller.go:624] quota admission added evaluator for: endpoints

* 
* ==> kube-apiserver [372b1bb82a5a] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0225 09:31:22.108313       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0225 09:31:22.108340       1 logging.go:59] [core] [Channel #5 SubChannel #6] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0225 09:31:22.108361       1 logging.go:59] [core] [Channel #22 SubChannel #23] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0225 09:31:22.108382       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0225 09:31:22.108408       1 logging.go:59] [core] [Channel #34 SubChannel #35] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0225 09:31:22.108433       1 logging.go:59] [core] [Channel #103 SubChannel #104] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0225 09:31:22.108459       1 logging.go:59] [core] [Channel #61 SubChannel #62] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [4a616b73cec7] <==
* I0225 09:23:09.333258       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0225 09:23:09.334413       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-create"
I0225 09:23:09.334685       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-patch"
I0225 09:23:09.336730       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-create"
I0225 09:23:09.336885       1 shared_informer.go:318] Caches are synced for namespace
I0225 09:23:09.338422       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0225 09:23:09.338807       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-patch"
I0225 09:23:09.339261       1 shared_informer.go:318] Caches are synced for TTL after finished
I0225 09:23:09.355068       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0225 09:23:09.356389       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0225 09:23:09.359128       1 shared_informer.go:318] Caches are synced for PVC protection
I0225 09:23:09.368504       1 shared_informer.go:318] Caches are synced for disruption
I0225 09:23:09.371822       1 shared_informer.go:318] Caches are synced for cronjob
I0225 09:23:09.376017       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0225 09:23:09.377146       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0225 09:23:09.378720       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0225 09:23:09.378840       1 shared_informer.go:318] Caches are synced for service account
I0225 09:23:09.378871       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0225 09:23:09.381495       1 shared_informer.go:318] Caches are synced for TTL
I0225 09:23:09.382775       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0225 09:23:09.387233       1 shared_informer.go:318] Caches are synced for HPA
I0225 09:23:09.387281       1 shared_informer.go:318] Caches are synced for crt configmap
I0225 09:23:09.389717       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0225 09:23:09.389811       1 shared_informer.go:318] Caches are synced for stateful set
I0225 09:23:09.389883       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="homework/web-deploy-5655fcd986" duration="140.508µs"
I0225 09:23:09.389941       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="40.032µs"
I0225 09:23:09.389975       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0225 09:23:09.390222       1 shared_informer.go:318] Caches are synced for expand
I0225 09:23:09.391693       1 shared_informer.go:318] Caches are synced for PV protection
I0225 09:23:09.417341       1 shared_informer.go:318] Caches are synced for node
I0225 09:23:09.417537       1 range_allocator.go:174] "Sending events to api server"
I0225 09:23:09.417739       1 range_allocator.go:178] "Starting range CIDR allocator"
I0225 09:23:09.417887       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0225 09:23:09.418001       1 shared_informer.go:318] Caches are synced for cidrallocator
I0225 09:23:09.420847       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0225 09:23:09.423715       1 shared_informer.go:318] Caches are synced for ephemeral
I0225 09:23:09.428944       1 shared_informer.go:318] Caches are synced for job
I0225 09:23:09.429133       1 shared_informer.go:318] Caches are synced for persistent volume
I0225 09:23:09.429582       1 shared_informer.go:318] Caches are synced for endpoint
I0225 09:23:09.430164       1 shared_informer.go:318] Caches are synced for ReplicationController
I0225 09:23:09.430325       1 shared_informer.go:318] Caches are synced for daemon sets
I0225 09:23:09.432882       1 shared_informer.go:318] Caches are synced for GC
I0225 09:23:09.435465       1 shared_informer.go:318] Caches are synced for deployment
I0225 09:23:09.453886       1 shared_informer.go:318] Caches are synced for resource quota
I0225 09:23:09.459083       1 shared_informer.go:318] Caches are synced for taint
I0225 09:23:09.459612       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0225 09:23:09.460278       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0225 09:23:09.460478       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0225 09:23:09.461288       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0225 09:23:09.460381       1 taint_manager.go:211] "Sending events to api server"
I0225 09:23:09.463756       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0225 09:23:09.513160       1 shared_informer.go:318] Caches are synced for resource quota
I0225 09:23:09.538695       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="148.822218ms"
I0225 09:23:09.539224       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="37.347µs"
I0225 09:23:09.589016       1 shared_informer.go:318] Caches are synced for attach detach
I0225 09:23:09.958795       1 shared_informer.go:318] Caches are synced for garbage collector
I0225 09:23:09.964995       1 shared_informer.go:318] Caches are synced for garbage collector
I0225 09:23:09.965069       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0225 09:23:15.631160       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="20.486807ms"
I0225 09:23:15.631228       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="36.244µs"

* 
* ==> kube-controller-manager [8cd9e6087dab] <==
* I0225 09:32:10.244245       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0225 09:32:10.244521       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="48.917µs"
I0225 09:32:10.248526       1 shared_informer.go:318] Caches are synced for ephemeral
I0225 09:32:10.253837       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="homework/web-deploy-5655fcd986" duration="9.507256ms"
I0225 09:32:10.253942       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="homework/web-deploy-5655fcd986" duration="78.142µs"
I0225 09:32:10.253975       1 shared_informer.go:318] Caches are synced for node
I0225 09:32:10.254058       1 range_allocator.go:174] "Sending events to api server"
I0225 09:32:10.254141       1 range_allocator.go:178] "Starting range CIDR allocator"
I0225 09:32:10.254198       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0225 09:32:10.254221       1 shared_informer.go:318] Caches are synced for cidrallocator
I0225 09:32:10.254876       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="10.531785ms"
I0225 09:32:10.254986       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="37.463µs"
I0225 09:32:10.259195       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0225 09:32:10.260691       1 shared_informer.go:318] Caches are synced for taint
I0225 09:32:10.260788       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0225 09:32:10.260860       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0225 09:32:10.260934       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0225 09:32:10.261038       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0225 09:32:10.261092       1 taint_manager.go:211] "Sending events to api server"
I0225 09:32:10.261221       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0225 09:32:10.265915       1 shared_informer.go:318] Caches are synced for job
I0225 09:32:10.268257       1 shared_informer.go:318] Caches are synced for stateful set
I0225 09:32:10.274945       1 shared_informer.go:318] Caches are synced for expand
I0225 09:32:10.276572       1 shared_informer.go:318] Caches are synced for service account
I0225 09:32:10.278308       1 shared_informer.go:318] Caches are synced for deployment
I0225 09:32:10.286354       1 shared_informer.go:318] Caches are synced for HPA
I0225 09:32:10.288633       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0225 09:32:10.288676       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0225 09:32:10.289893       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0225 09:32:10.291496       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0225 09:32:10.291531       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0225 09:32:10.294044       1 shared_informer.go:318] Caches are synced for PV protection
I0225 09:32:10.296529       1 shared_informer.go:318] Caches are synced for endpoint
I0225 09:32:10.298246       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0225 09:32:10.299748       1 shared_informer.go:318] Caches are synced for crt configmap
I0225 09:32:10.301723       1 shared_informer.go:318] Caches are synced for PVC protection
I0225 09:32:10.301776       1 shared_informer.go:318] Caches are synced for TTL after finished
I0225 09:32:10.301785       1 shared_informer.go:318] Caches are synced for persistent volume
I0225 09:32:10.302933       1 shared_informer.go:318] Caches are synced for ReplicationController
I0225 09:32:10.306252       1 shared_informer.go:318] Caches are synced for cronjob
I0225 09:32:10.306378       1 shared_informer.go:318] Caches are synced for GC
I0225 09:32:10.307644       1 shared_informer.go:318] Caches are synced for TTL
I0225 09:32:10.307680       1 shared_informer.go:318] Caches are synced for daemon sets
I0225 09:32:10.310954       1 shared_informer.go:318] Caches are synced for disruption
I0225 09:32:10.311068       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0225 09:32:10.312250       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0225 09:32:10.392943       1 shared_informer.go:318] Caches are synced for attach detach
I0225 09:32:10.489864       1 shared_informer.go:318] Caches are synced for resource quota
I0225 09:32:10.515004       1 shared_informer.go:318] Caches are synced for resource quota
I0225 09:32:10.709317       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="homework/web-deploy-5655fcd986" duration="6.977236ms"
I0225 09:32:10.710139       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="homework/web-deploy-5655fcd986" duration="20.299µs"
I0225 09:32:10.839258       1 shared_informer.go:318] Caches are synced for garbage collector
I0225 09:32:10.839354       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0225 09:32:10.840678       1 shared_informer.go:318] Caches are synced for garbage collector
I0225 09:32:10.962731       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="homework/web-deploy-5655fcd986" duration="9.693732ms"
I0225 09:32:10.963121       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="homework/web-deploy-5655fcd986" duration="43.989µs"
I0225 09:32:10.975547       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="homework/web-deploy-5655fcd986" duration="11.4422ms"
I0225 09:32:10.975679       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="homework/web-deploy-5655fcd986" duration="40.015µs"
I0225 09:32:15.906623       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="11.994761ms"
I0225 09:32:15.907856       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="37.866µs"

* 
* ==> kube-proxy [5e706d51b2c2] <==
* I0225 09:22:58.329646       1 server_others.go:69] "Using iptables proxy"
I0225 09:22:58.356453       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0225 09:22:58.389987       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0225 09:22:58.399735       1 server_others.go:152] "Using iptables Proxier"
I0225 09:22:58.399820       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0225 09:22:58.399827       1 server_others.go:438] "Defaulting to no-op detect-local"
I0225 09:22:58.399848       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0225 09:22:58.399984       1 server.go:846] "Version info" version="v1.28.3"
I0225 09:22:58.399991       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0225 09:22:58.400773       1 config.go:188] "Starting service config controller"
I0225 09:22:58.400778       1 shared_informer.go:311] Waiting for caches to sync for service config
I0225 09:22:58.400788       1 config.go:97] "Starting endpoint slice config controller"
I0225 09:22:58.400791       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0225 09:22:58.400969       1 config.go:315] "Starting node config controller"
I0225 09:22:58.400975       1 shared_informer.go:311] Waiting for caches to sync for node config
I0225 09:22:58.521254       1 shared_informer.go:318] Caches are synced for node config
I0225 09:22:58.521274       1 shared_informer.go:318] Caches are synced for service config
I0225 09:22:58.521295       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-proxy [bc995854f076] <==
* I0225 09:31:59.704526       1 server_others.go:69] "Using iptables proxy"
I0225 09:31:59.718123       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0225 09:31:59.803274       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0225 09:31:59.816574       1 server_others.go:152] "Using iptables Proxier"
I0225 09:31:59.816708       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0225 09:31:59.816798       1 server_others.go:438] "Defaulting to no-op detect-local"
I0225 09:31:59.816894       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0225 09:31:59.818691       1 server.go:846] "Version info" version="v1.28.3"
I0225 09:31:59.818703       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0225 09:31:59.819459       1 config.go:188] "Starting service config controller"
I0225 09:31:59.819465       1 shared_informer.go:311] Waiting for caches to sync for service config
I0225 09:31:59.819474       1 config.go:97] "Starting endpoint slice config controller"
I0225 09:31:59.819476       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0225 09:31:59.819664       1 config.go:315] "Starting node config controller"
I0225 09:31:59.819675       1 shared_informer.go:311] Waiting for caches to sync for node config
I0225 09:31:59.921075       1 shared_informer.go:318] Caches are synced for node config
I0225 09:31:59.921095       1 shared_informer.go:318] Caches are synced for service config
I0225 09:31:59.921112       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-scheduler [67700306ce16] <==
* I0225 09:22:54.972545       1 serving.go:348] Generated self-signed cert in-memory
W0225 09:22:56.256574       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0225 09:22:56.256593       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0225 09:22:56.256600       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0225 09:22:56.256604       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0225 09:22:56.281586       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0225 09:22:56.281606       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0225 09:22:56.284754       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0225 09:22:56.284775       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0225 09:22:56.285113       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0225 09:22:56.285199       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0225 09:22:56.385121       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0225 09:31:22.048017       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0225 09:31:22.048062       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
E0225 09:31:22.048179       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kube-scheduler [8d8b5a8d40fd] <==
* I0225 09:31:56.166216       1 serving.go:348] Generated self-signed cert in-memory
W0225 09:31:57.982464       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0225 09:31:57.982485       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0225 09:31:57.982492       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0225 09:31:57.982497       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0225 09:31:58.001167       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0225 09:31:58.001185       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0225 09:31:58.002314       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0225 09:31:58.002390       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0225 09:31:58.002403       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0225 09:31:58.002412       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0225 09:31:58.102833       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Feb 25 09:31:54 minikube kubelet[1372]: I0225 09:31:54.760145    1372 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/55b4bbe24dac3803a7379f9ae169d6ba-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"55b4bbe24dac3803a7379f9ae169d6ba\") " pod="kube-system/kube-apiserver-minikube"
Feb 25 09:31:54 minikube kubelet[1372]: I0225 09:31:54.760157    1372 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/55b4bbe24dac3803a7379f9ae169d6ba-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"55b4bbe24dac3803a7379f9ae169d6ba\") " pod="kube-system/kube-apiserver-minikube"
Feb 25 09:31:54 minikube kubelet[1372]: I0225 09:31:54.760168    1372 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/55b4bbe24dac3803a7379f9ae169d6ba-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"55b4bbe24dac3803a7379f9ae169d6ba\") " pod="kube-system/kube-apiserver-minikube"
Feb 25 09:31:54 minikube kubelet[1372]: I0225 09:31:54.760181    1372 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/7da72fc2e2cfb27aacf6cffd1c72da00-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"7da72fc2e2cfb27aacf6cffd1c72da00\") " pod="kube-system/kube-controller-manager-minikube"
Feb 25 09:31:54 minikube kubelet[1372]: E0225 09:31:54.766254    1372 remote_runtime.go:385] "RemoveContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to remove container \"2c59d6a2b6ddecf2f2131d73d7bcda5f62475cb2c2699d1c3ecba77d34a34796\": Error response from daemon: removal of container 2c59d6a2b6ddecf2f2131d73d7bcda5f62475cb2c2699d1c3ecba77d34a34796 is already in progress" containerID="2c59d6a2b6ddecf2f2131d73d7bcda5f62475cb2c2699d1c3ecba77d34a34796"
Feb 25 09:31:54 minikube kubelet[1372]: I0225 09:31:54.766350    1372 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"2c59d6a2b6ddecf2f2131d73d7bcda5f62475cb2c2699d1c3ecba77d34a34796"} err="rpc error: code = Unknown desc = failed to remove container \"2c59d6a2b6ddecf2f2131d73d7bcda5f62475cb2c2699d1c3ecba77d34a34796\": Error response from daemon: removal of container 2c59d6a2b6ddecf2f2131d73d7bcda5f62475cb2c2699d1c3ecba77d34a34796 is already in progress"
Feb 25 09:31:54 minikube kubelet[1372]: I0225 09:31:54.766403    1372 scope.go:117] "RemoveContainer" containerID="99a09d54df5f1e77fd3db7203ff0be6113b57a94753b8da1ad3ced2fed1cdfd9"
Feb 25 09:31:54 minikube kubelet[1372]: E0225 09:31:54.766956    1372 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 99a09d54df5f1e77fd3db7203ff0be6113b57a94753b8da1ad3ced2fed1cdfd9" containerID="99a09d54df5f1e77fd3db7203ff0be6113b57a94753b8da1ad3ced2fed1cdfd9"
Feb 25 09:31:54 minikube kubelet[1372]: I0225 09:31:54.767024    1372 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"99a09d54df5f1e77fd3db7203ff0be6113b57a94753b8da1ad3ced2fed1cdfd9"} err="failed to get container status \"99a09d54df5f1e77fd3db7203ff0be6113b57a94753b8da1ad3ced2fed1cdfd9\": rpc error: code = Unknown desc = Error response from daemon: No such container: 99a09d54df5f1e77fd3db7203ff0be6113b57a94753b8da1ad3ced2fed1cdfd9"
Feb 25 09:31:54 minikube kubelet[1372]: I0225 09:31:54.767067    1372 scope.go:117] "RemoveContainer" containerID="99a09d54df5f1e77fd3db7203ff0be6113b57a94753b8da1ad3ced2fed1cdfd9"
Feb 25 09:31:54 minikube kubelet[1372]: I0225 09:31:54.767527    1372 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"99a09d54df5f1e77fd3db7203ff0be6113b57a94753b8da1ad3ced2fed1cdfd9"} err="failed to get container status \"99a09d54df5f1e77fd3db7203ff0be6113b57a94753b8da1ad3ced2fed1cdfd9\": rpc error: code = Unknown desc = Error response from daemon: No such container: 99a09d54df5f1e77fd3db7203ff0be6113b57a94753b8da1ad3ced2fed1cdfd9"
Feb 25 09:31:54 minikube kubelet[1372]: I0225 09:31:54.767610    1372 scope.go:117] "RemoveContainer" containerID="4aa71685d627a4067d37d90e108f20e797353fb933535823d5004bd0d239fd97"
Feb 25 09:31:54 minikube kubelet[1372]: E0225 09:31:54.768104    1372 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 4aa71685d627a4067d37d90e108f20e797353fb933535823d5004bd0d239fd97" containerID="4aa71685d627a4067d37d90e108f20e797353fb933535823d5004bd0d239fd97"
Feb 25 09:31:54 minikube kubelet[1372]: I0225 09:31:54.768187    1372 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"4aa71685d627a4067d37d90e108f20e797353fb933535823d5004bd0d239fd97"} err="failed to get container status \"4aa71685d627a4067d37d90e108f20e797353fb933535823d5004bd0d239fd97\": rpc error: code = Unknown desc = Error response from daemon: No such container: 4aa71685d627a4067d37d90e108f20e797353fb933535823d5004bd0d239fd97"
Feb 25 09:31:54 minikube kubelet[1372]: I0225 09:31:54.768247    1372 scope.go:117] "RemoveContainer" containerID="4aa71685d627a4067d37d90e108f20e797353fb933535823d5004bd0d239fd97"
Feb 25 09:31:54 minikube kubelet[1372]: I0225 09:31:54.768701    1372 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"4aa71685d627a4067d37d90e108f20e797353fb933535823d5004bd0d239fd97"} err="failed to get container status \"4aa71685d627a4067d37d90e108f20e797353fb933535823d5004bd0d239fd97\": rpc error: code = Unknown desc = Error response from daemon: No such container: 4aa71685d627a4067d37d90e108f20e797353fb933535823d5004bd0d239fd97"
Feb 25 09:31:54 minikube kubelet[1372]: I0225 09:31:54.878701    1372 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Feb 25 09:31:54 minikube kubelet[1372]: E0225 09:31:54.879468    1372 kubelet_node_status.go:92] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Feb 25 09:31:55 minikube kubelet[1372]: E0225 09:31:55.159986    1372 controller.go:146] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="800ms"
Feb 25 09:31:55 minikube kubelet[1372]: I0225 09:31:55.300899    1372 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Feb 25 09:31:55 minikube kubelet[1372]: E0225 09:31:55.301116    1372 kubelet_node_status.go:92] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Feb 25 09:31:55 minikube kubelet[1372]: I0225 09:31:55.806463    1372 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="f5c355693bd021dbeb75b4bfb0c391d035f4372979d333da4a1b9249cb31bb53"
Feb 25 09:31:56 minikube kubelet[1372]: I0225 09:31:56.119287    1372 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.084319    1372 kubelet_node_status.go:108] "Node was previously registered" node="minikube"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.084681    1372 kubelet_node_status.go:73] "Successfully registered node" node="minikube"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.085707    1372 kuberuntime_manager.go:1523] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.086170    1372 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.539885    1372 apiserver.go:52] "Watching apiserver"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.551099    1372 topology_manager.go:215] "Topology Admit Handler" podUID="4f4796b2-7736-451c-8980-4101c03710de" podNamespace="kube-system" podName="storage-provisioner"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.551482    1372 topology_manager.go:215] "Topology Admit Handler" podUID="5d8bb303-bddf-441c-91b5-796fff7af79d" podNamespace="kube-system" podName="kube-proxy-cfj6z"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.551970    1372 topology_manager.go:215] "Topology Admit Handler" podUID="3476c5b5-72c2-4358-aa08-c9d6f7c7ba94" podNamespace="kube-system" podName="coredns-5dd5756b68-hdxcg"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.552436    1372 topology_manager.go:215] "Topology Admit Handler" podUID="92674c64-de09-49a1-b5d9-a166d2f3ad2a" podNamespace="ingress-nginx" podName="ingress-nginx-admission-patch-d99lv"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.552841    1372 topology_manager.go:215] "Topology Admit Handler" podUID="25399ab7-40df-48a9-8b42-4c78d0d05e2e" podNamespace="ingress-nginx" podName="ingress-nginx-controller-7c6974c4d8-8lfbr"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.554737    1372 topology_manager.go:215] "Topology Admit Handler" podUID="30519fe9-59b1-439e-bb1d-039f9985918c" podNamespace="ingress-nginx" podName="ingress-nginx-admission-create-wm8b9"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.555014    1372 topology_manager.go:215] "Topology Admit Handler" podUID="66473035-0127-4858-a97c-9df2c309146c" podNamespace="homework" podName="web-deploy-5655fcd986-kfqrq"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.555226    1372 topology_manager.go:215] "Topology Admit Handler" podUID="2723b09d-ddde-40fa-a5e9-f389af4b85d4" podNamespace="homework" podName="web-deploy-5655fcd986-6lctw"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.555393    1372 topology_manager.go:215] "Topology Admit Handler" podUID="ca3ee18e-3504-4610-a240-079b527db6e7" podNamespace="homework" podName="web-deploy-5655fcd986-pklvd"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.660655    1372 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.691325    1372 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/5d8bb303-bddf-441c-91b5-796fff7af79d-lib-modules\") pod \"kube-proxy-cfj6z\" (UID: \"5d8bb303-bddf-441c-91b5-796fff7af79d\") " pod="kube-system/kube-proxy-cfj6z"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.691490    1372 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-11112082-97d4-4ee6-b366-3b88c5e152ff\" (UniqueName: \"kubernetes.io/host-path/66473035-0127-4858-a97c-9df2c309146c-pvc-11112082-97d4-4ee6-b366-3b88c5e152ff\") pod \"web-deploy-5655fcd986-kfqrq\" (UID: \"66473035-0127-4858-a97c-9df2c309146c\") " pod="homework/web-deploy-5655fcd986-kfqrq"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.691562    1372 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/4f4796b2-7736-451c-8980-4101c03710de-tmp\") pod \"storage-provisioner\" (UID: \"4f4796b2-7736-451c-8980-4101c03710de\") " pod="kube-system/storage-provisioner"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.691624    1372 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-11112082-97d4-4ee6-b366-3b88c5e152ff\" (UniqueName: \"kubernetes.io/host-path/2723b09d-ddde-40fa-a5e9-f389af4b85d4-pvc-11112082-97d4-4ee6-b366-3b88c5e152ff\") pod \"web-deploy-5655fcd986-6lctw\" (UID: \"2723b09d-ddde-40fa-a5e9-f389af4b85d4\") " pod="homework/web-deploy-5655fcd986-6lctw"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.691696    1372 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/5d8bb303-bddf-441c-91b5-796fff7af79d-xtables-lock\") pod \"kube-proxy-cfj6z\" (UID: \"5d8bb303-bddf-441c-91b5-796fff7af79d\") " pod="kube-system/kube-proxy-cfj6z"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.691773    1372 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-11112082-97d4-4ee6-b366-3b88c5e152ff\" (UniqueName: \"kubernetes.io/host-path/ca3ee18e-3504-4610-a240-079b527db6e7-pvc-11112082-97d4-4ee6-b366-3b88c5e152ff\") pod \"web-deploy-5655fcd986-pklvd\" (UID: \"ca3ee18e-3504-4610-a240-079b527db6e7\") " pod="homework/web-deploy-5655fcd986-pklvd"
Feb 25 09:31:58 minikube kubelet[1372]: I0225 09:31:58.997392    1372 scope.go:117] "RemoveContainer" containerID="7d135ae7382404c37a5e8f8a52f8e47fcf162dd685d16069c5d2484b3f221c3a"
Feb 25 09:31:59 minikube kubelet[1372]: I0225 09:31:59.020537    1372 scope.go:117] "RemoveContainer" containerID="651f1491025b9b4190f4db5bf0f44162cd307f6d61c3e708cd0233ff4fe68991"
Feb 25 09:31:59 minikube kubelet[1372]: I0225 09:31:59.020606    1372 scope.go:117] "RemoveContainer" containerID="5ea01f73f61d8ef4330ff26d82a6cf1c7258f3354555630e1232a56b9ca523b6"
Feb 25 09:32:00 minikube kubelet[1372]: I0225 09:32:00.140037    1372 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="ca9f1d3588698852feefe5b1fbbe6b0008d58dbe41987f5fe553c00a74e13f90"
Feb 25 09:32:00 minikube kubelet[1372]: I0225 09:32:00.194490    1372 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="5163d2d558343d3021c2e331849f86660b8cf977a0d6edb41af851d338a8c460"
Feb 25 09:32:00 minikube kubelet[1372]: I0225 09:32:00.226277    1372 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="73d25e583cef85677d9f5bcf39ef735511ea66d9ee97bd6fe1a43950f8aae088"
Feb 25 09:32:01 minikube kubelet[1372]: I0225 09:32:01.358891    1372 scope.go:117] "RemoveContainer" containerID="7aa72602f1be5fef8faf475a7c67c5558ced476b6ca8c68fc898ad6501d385a4"
Feb 25 09:32:02 minikube kubelet[1372]: I0225 09:32:02.415130    1372 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Feb 25 09:32:02 minikube kubelet[1372]: I0225 09:32:02.415130    1372 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Feb 25 09:32:03 minikube kubelet[1372]: I0225 09:32:03.471953    1372 scope.go:117] "RemoveContainer" containerID="2d9d8fee0d876deef5a9e5a3a066b570b3e9dfa797ea237cca5059ede865d6d2"
Feb 25 09:32:04 minikube kubelet[1372]: I0225 09:32:04.540489    1372 scope.go:117] "RemoveContainer" containerID="7ed6fd081b516ddb3c793c2b3d38f4de09d9266249ad7d1ee9a3df156d00b053"
Feb 25 09:32:04 minikube kubelet[1372]: I0225 09:32:04.875093    1372 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Feb 25 09:32:30 minikube kubelet[1372]: I0225 09:32:30.315291    1372 scope.go:117] "RemoveContainer" containerID="a61056ae36504c4e44870cbe0d7d2175711949c10c602ad7cb9ede11e3357eb3"
Feb 25 09:32:30 minikube kubelet[1372]: I0225 09:32:30.315774    1372 scope.go:117] "RemoveContainer" containerID="e4e5f661a4f9adbfdd63b0417a8fc61b77df0ae8f2868da79dec682021c26c80"
Feb 25 09:32:30 minikube kubelet[1372]: E0225 09:32:30.315940    1372 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(4f4796b2-7736-451c-8980-4101c03710de)\"" pod="kube-system/storage-provisioner" podUID="4f4796b2-7736-451c-8980-4101c03710de"
Feb 25 09:32:44 minikube kubelet[1372]: I0225 09:32:44.584211    1372 scope.go:117] "RemoveContainer" containerID="e4e5f661a4f9adbfdd63b0417a8fc61b77df0ae8f2868da79dec682021c26c80"

* 
* ==> storage-provisioner [e4e5f661a4f9] <==
* I0225 09:31:59.617934       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0225 09:32:29.619575       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

